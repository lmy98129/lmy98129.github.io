<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">











<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.0.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.4">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.4" color="#222">







<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Gemini',
    version: '6.0.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  
  <meta name="keywords" content="experience," />


<meta name="description" content="本文是学习Neural ODE过程中的第三篇个人笔记 主要是SDE随机微分方程基本原理、基于高斯过程的近似方法以及在状态估计上的应用，包括2023年Nature和NeurIPS上的两篇同团队论文。">
<meta property="og:type" content="article">
<meta property="og:title" content="Notes About Neural ODE and Beyond 3">
<meta property="og:url" content="http://lmy98129.github.io/2024/01/10/Notes-About-Neural-ODE-and-Beyond-3/index.html">
<meta property="og:site_name" content="NeXT">
<meta property="og:description" content="本文是学习Neural ODE过程中的第三篇个人笔记 主要是SDE随机微分方程基本原理、基于高斯过程的近似方法以及在状态估计上的应用，包括2023年Nature和NeurIPS上的两篇同团队论文。">
<meta property="og:locale">
<meta property="og:image" content="https://github.com/rtqichen/torchdiffeq/blob/master/assets/ode_demo.gif?raw=true">
<meta property="og:image" content="https://www.wolfram.com/mathematica/new-in-9/time-series-and-stochastic-differential-equations/HTMLImages.en/stochastic-differential-equation-for-exponential-d/O_49.png">
<meta property="article:published_time" content="2024-01-10T05:37:07.000Z">
<meta property="article:modified_time" content="2024-01-11T13:46:49.713Z">
<meta property="article:author" content="Mengyin Liu">
<meta property="article:tag" content="experience">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/rtqichen/torchdiffeq/blob/master/assets/ode_demo.gif?raw=true">






  <link rel="canonical" href="http://lmy98129.github.io/2024/01/10/Notes-About-Neural-ODE-and-Beyond-3/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>
  <title>Notes About Neural ODE and Beyond 3 | NeXT</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

<meta name="generator" content="Hexo 5.4.2"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">NeXT</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
        </li>
      
        
        <li class="menu-item menu-item-academic">
          <a href="/academic/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-flask"></i> <br />学术</a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />标签</a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>
        </li>
      

      
    </ul>
  

  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://lmy98129.github.io/2024/01/10/Notes-About-Neural-ODE-and-Beyond-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mengyin Liu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NeXT">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Notes About Neural ODE and Beyond 3</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2024-01-10T13:37:07+08:00">2024-01-10</time>
            

            
            
              
                
              
            

            
              
              <span class="post-meta-divider">|</span>
              

              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2024-01-11T21:46:49+08:00">2024-01-11</time>
            
          </span>

          

          
            
          

          
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文是学习Neural ODE过程中的第三篇个人笔记</p>
<p>主要是SDE随机微分方程基本原理、基于高斯过程的近似方法<br>以及在状态估计上的应用，包括2023年Nature和NeurIPS上的两篇同团队论文。</p>
<span id="more"></span>
<h2 id="勘误和一些推论-Corrigendum-and-Corollaries"><a href="#勘误和一些推论-Corrigendum-and-Corollaries" class="headerlink" title="勘误和一些推论 Corrigendum and Corollaries"></a>勘误和一些推论 Corrigendum and Corollaries</h2><p>首先需要更正一下，其实Neural ODE也支持直接用中间的隐式状态表示预测结果，不需要额外的网络来解码隐状态。可以参见陈天琦团队的github仓库<sup class="refplus-num"><a href="#ref-torchdiffeq">[1]</a></sup>的<a target="_blank" rel="noopener" href="https://github.com/rtqichen/torchdiffeq/blob/master/examples/ode_demo.py#L166">简单示例</a>。由于这个示例需要拟合的是下图螺旋运动轨迹方程，只有两个变量：运动点的2D坐标$x$和$y$，而且目标方程也不复杂，只有几个参数，所以直接让神经网络接受上一次输出的$(x, y)^\top$，再输出下一次的$(x, y)^\top$，不需要复杂的隐藏变量$z(t)$，也能正确拟合这个方程。</p>
<p><img src="https://github.com/rtqichen/torchdiffeq/blob/master/assets/ode_demo.gif?raw=true" alt="ode-demo"></p>
<center>Neural ODE的简单示例动画，动画的每一帧是一轮学习后的测试结果。<br>左图是这个螺旋曲线的动点在不同时刻$t$下在$x$轴和$y$的坐标变化序列，虚线是预测序列，实线是目标序列。右图是这个螺旋曲线经过所有时间后，在2D的$x-y$坐标系下运动的轨迹，虚线是预测轨迹，实线是目标轨迹。<br>图片来自torchdiffeq官方github仓库<sup class="refplus-num"><a href="#ref-torchdiffeq">[1]</a></sup></center>

<p><br></p>
<p>而且，如果目标方程确定的话，实际上就让网络拟合这一个方程就可以了，不需要任何的额外输入（除了初始状态$(x_0, y_0)^\top$），所以也就不需要像Neural ODE<sup class="refplus-num"><a href="#ref-Neural-ODE">[2]</a></sup>或者Latent ODE<sup class="refplus-num"><a href="#ref-Latent-ODE">[3]</a></sup>原文里的RNN、ODE-RNN作为编码器（文章里面经常称为Recognition Model，应该是领域术语），生成隐状态服从的高斯分布$z(t_0) \sim \mathcal{N}(\mu, \sigma)$的均值和方差。</p>
<p>$$<br>\begin{equation}<br>z(t+\epsilon) = z(t) + \int_{t}^{t+\epsilon} f(z(t’), t’, \theta)dt’<br>\end{equation}<br>$$</p>
<p>有趣的是，Neural ODE为了和ODE保持一致，像上面的式子一样，能够不断地在同一个微分函数上积分，然后与初值相加，确实是不允许输入和输出的$z$形状不一致的（容易联想到Diffusion扩散模型的基本原理和ODE有联系，这就是另一些故事了，感兴趣可以去搜相应的工作）。也就是说，不允许$z(t_0)$是一个隐式的特征，而剩下的$z(t&gt;t_0)$是显式的预测值。所以，凡是使用了隐式特征的Neural ODE，都需要一个额外的解码器，可以参见<a target="_blank" rel="noopener" href="https://github.com/rtqichen/torchdiffeq/blob/master/examples/latent_ode.py#L263">这个示例代码</a>。</p>
<p>所以，从某种意义上来说，前面几篇笔记反复提到的“错误”也不全错，对复杂过程的建模，而且需要一个编码器生成初始状态作为条件condition，就是需要在隐式特征的状态上做积分。但是，如果解码器$\phi$是线性的且没有偏置，那么输出仍然服从和隐式状态一样的变化量关系约束。由于对任意$t$都有$y(t)=\phi(z(t))$：</p>
<p>$$<br>\begin{align}<br>&amp; y(t+\epsilon) = \phi(z(t+\epsilon)) \nonumber \\<br>&amp; = \phi\left(z(t) + \int_{t}^{t+\epsilon} f(z(t’), t’, \theta)dt’\right) \nonumber \\<br>&amp; = \phi(z(t)) + \phi\left(\int_{t}^{t+\epsilon} f(z(t’), t’, \theta)dt’\right) \nonumber \\<br>&amp; = y(t) + \Delta y|_t^{t+\epsilon} \\<br>\end{align}<br>$$</p>
<p>其中，最后这项$\phi(\cdot)$可以记作$\Delta y|_t^{t+\epsilon}$，也就是$\epsilon$长度时间内$y$的变化量，当然具体它是否真的像式子（1）是个积分，$\Delta y|_t^{t+\epsilon} = \int_t^{t+\epsilon} \frac{dy}{dt} dt$，就看网络拟合的程度了，至少它相对之前是个变化量，有物理意义。为什么非线性的解码器不能做到这点呢？最简单的例子就是指数函数：$e^{(a+b)} = e^a \cdot e^b \neq e^a + e^b$，$a$和$b$相加后再输入的结果，不能等于$a$和$b$分别输入再相加的结果。比较遗憾的是，Neural ODE的默认解码器是<a target="_blank" rel="noopener" href="https://github.com/rtqichen/torchdiffeq/blob/master/examples/latent_ode.py#L157">非线性的</a>，所以甚至不能满足式子（2）的关系。</p>
<p>当然，可以强行解读为$y(t+\epsilon) - y(t)=\phi(z(t+\epsilon)) - \phi(z(t))$就是$\Delta y|_t^{t+\epsilon}$，是网络学出来的。但是无法像式子（1），由固定的$f = \frac{dy}{dt}$积分得到，相比黑盒$\Delta y$更加可解释一些。以上都是我个人的推导，难免还有错误，仅供参考。</p>
<h2 id="随机微分方程-Stochastic-Differential-Equation"><a href="#随机微分方程-Stochastic-Differential-Equation" class="headerlink" title="随机微分方程 Stochastic Differential Equation"></a>随机微分方程 Stochastic Differential Equation</h2><p>如果有观察过Neural ODE<sup class="refplus-num"><a href="#ref-Neural-ODE">[2]</a></sup>和Latent ODE<sup class="refplus-num"><a href="#ref-Latent-ODE">[3]</a></sup>原文的实验部分，可以发现它们的拟合的时间序列往往是平滑形式的。但是，很多真实的物理过程是存在大量随机的，例如流体运动、热量传递等，底层都是分子原子的布朗运动（Brownian Motion）。只要输入相同，微分方程$f(\cdot)$输出一般没有随机性。当然，其基于VAE的思想确实能够通过对若干次初始状态$z(t_0) \sim \mathcal{N}(\mu, \sigma)$的采样，最后对预测结果算均值来一定程度上缓解这个问题，但是最终的曲线趋势仍然是平滑的。</p>
<p>以下推导直接参考剑桥大学2019年出版的教材“Applied Stochastic Differential Equations”<sup class="refplus-num"><a href="#ref-Applied-SDE">[5]</a></sup>（Kevin Course和Prasanth B. Nair在2023年发表的Nature正刊<sup class="refplus-num"><a href="#ref-Nature-State">[6]</a></sup>和NeurIPS正会<sup class="refplus-num"><a href="#ref-NeurIPS-State">[7]</a></sup>论文就参考了这个教材）。</p>
<p>为了描述一个带有噪声过程$w(t)$的隐式状态$x(t)$变化过程，我们将常微分方程$\frac{dx(t)}{dt} = f(x(t), t)$改写为：</p>
<p>$$<br>\begin{equation}<br>\frac{dx(t)}{dt} = f(x(t), t) + L(x(t), t)w(t)<br>\end{equation}<br>$$</p>
<p>其中，随机过程$w(t)$本身与状态$x(t)$无关，一般是白噪声过程（White Noise），例如外界随机影响、内部随机运动。而$L(x(t), t)$和$f$一样，是一个与$x(t)$有关的任意函数（也可以与$x(t)$无关，带$x(t)$的项目就为0）。</p>
<p>那么，和<a href="https://lmy98129.github.io/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/#%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B-Ordinary-Differential-Equation">之前的笔记</a>一样，给定初值$x(t_0)$，对随机微分方程式子（3）求积分得到任意$t$时刻下的$x(t)$：</p>
<p>$$<br>\begin{align}<br>&amp; x(t) - x(t_0) \nonumber \\<br>&amp; = \int_{t_0}^{t} f(x(t), t) dt + \int_{t_0}^{t} L(x(t), t)w(t) dt \\<br>\end{align}<br>$$</p>
<p>显然，由于第一个积分是确定性的，无论是求解析解还是数值解，都是可行的方案。问题比较大的是第二个积分，因为它带有一个随机过程$w(t)$.。将这个积分写成极限形式：</p>
<p>$$<br>\begin{align}<br>&amp; \int_{t_0}^{t} L(x(t), t)w(t) dt \nonumber \\<br>&amp; = \lim_{n \rightarrow \infty} \sum_k L(x(t^*_k), t^*_k)w(t^*_k)(t_{k+1} - t_k), \nonumber \\<br>&amp; \mathrm{where} \nonumber \\<br>&amp; t_0 &lt; t_1 &lt; \cdots &lt; t_n = t, ~~ t^*_k \in [t_k, t_{k+1}]<br>\end{align}<br>$$</p>
<p>简而言之，将$[t_0, t]$划成$n \rightarrow \infty$个小区间，在任意两个区间之间计算矩形面积，宽为$t_{k+1} - t_{k}$，高为这个区间中取一个点$t^*_k$对应的函数值$L(x(t^*_k), t^*_k)w(t^*_k)$，将所有矩形面积相加即为积分。然而，由于随机过程$w(t^*_k)$并没有上下界，所以这样取一个或多个$t^*_k \in [t_k, t_{k+1}]$代表小区间矩形高度是没有意义的（形象一点就是一会儿很高、一会儿很低）。</p>
<p>为了保证这个代表小区间高度的被积函数是有界的，我们需要尝试将白噪声过程$w(t)$替换为有界的随机过程。布朗运动（Brownian Motion）就比较合适，$w(t)dt = d\beta(t) \sim \mathcal{N}(0, Qdt)$，$Q$为方差的扩散系数，满足以下的性质：</p>
<ul>
<li>给定时间区间$\Delta t_k = t_{k+1} - t_k$，$\Delta \beta_k = \beta(t_{k+1}) - \beta(t_k) \sim \mathcal{N}(0, Q\Delta t_k)$都是一个有界的高斯分布。</li>
<li>初始时间的$\beta(t_0)=0$，且在不重叠的时间段，$\Delta \beta_k$的值是相互独立的，不会被前后的差值影响。</li>
</ul>
<p>由此，我们可以代入$w(t)dt = d\beta(t)$，重写式子（5）为：</p>
<p>$$<br>\begin{align}<br>&amp; \int_{t_0}^{t} L(x(t), t)d\beta(t) \nonumber \\<br>&amp; = \lim_{n \rightarrow \infty} \sum_k L(x(t^*_k), t^*_k)(\beta(t_{k+1}) - \beta(t_k)), \nonumber \\<br>&amp; \mathrm{where} \nonumber \\<br>&amp; t_0 &lt; t_1 &lt; \cdots &lt; t_n = t, ~~ t^*_k \in [t_k, t_{k+1}] \nonumber \\<br>\end{align}<br>$$</p>
<p>但是，在区间中取不同的点$t^*_k \in [t_k, t_{k+1}]$，对于函数$L(x(t^*_k), t^*_k)$取值也有影响，最终影响积分值。因此，在这里直接采用伊藤随机积分（Itô Stochastic Integral），取$t=t_k$，保证最终的积分值是唯一的。</p>
<p>$$<br>\begin{align}<br>&amp; \int_{t_0}^{t} L(x(t), t)d\beta(t) \nonumber \\<br>&amp; = \lim_{n \rightarrow \infty} \sum_k L(x(t_k), t_k)(\beta(t_{k+1}) - \beta(t_k)), \nonumber \\<br>&amp; \mathrm{where} ~ t_0 &lt; t_1 &lt; \cdots &lt; t_n = t<br>\end{align}<br>$$</p>
<p>最终，式子（4）可以重写为：</p>
<p>$$<br>\begin{align}<br>&amp; x(t) - x(t_0) \nonumber \\<br>&amp; = \int_{t_0}^{t} f(x(t), t) dt + \int_{t_0}^{t} L(x(t), t)d\beta(t) \nonumber \\<br>\end{align}<br>$$</p>
<p>相应的随机微分方程，被称为伊藤随机微分方程（Itô SDE）：</p>
<p>$$<br>\begin{align}<br>&amp; \frac{dx(t)}{dt} = f(x(t), t) + L(x(t), t)\frac{d\beta(t)}{dt} \nonumber \\<br>&amp; \Rightarrow dx(t) = f(x(t), t)dt + L(x(t), t)d\beta(t)<br>\end{align}<br>$$</p>
<p>其中，函数$f(\cdot)$被称为漂移函数（Drift Function），因为它决定了整个变化过程的总体趋势。函数$L(\cdot)$被成为扩散函数（Diffusion Function），因为它基于布朗运动，决定了随机运动（类似于粒子扩散）的幅度。</p>
<p><img src="https://www.wolfram.com/mathematica/new-in-9/time-series-and-stochastic-differential-equations/HTMLImages.en/stochastic-differential-equation-for-exponential-d/O_49.png" alt="wolfram"></p>
<center>设置不同的扩散函数系数$\sigma$后，多次采样SDE的曲线。图片来自<a target="_blank" rel="noopener" href="https://www.wolfram.com/mathematica/new-in-9/time-series-and-stochastic-differential-equations/stochastic-differential-equation-for-exponential-d.html">Mathematica官网</a></center>

<h2 id="高斯过程近似-Gaussian-Process-Approximation-of-SDE-Solutions"><a href="#高斯过程近似-Gaussian-Process-Approximation-of-SDE-Solutions" class="headerlink" title="高斯过程近似 Gaussian Process Approximation of SDE Solutions"></a>高斯过程近似 Gaussian Process Approximation of SDE Solutions</h2><p>在Kevin Course和Prasanth B. Nair的两篇论文<sup class="refplus-num"><a href="#ref-Nature-State">[6]</a></sup><sup class="refplus-num"><a href="#ref-NeurIPS-State">[7]</a></sup>中，都提到了可以通过高斯过程（Gaussian Process）来近似解随机微分方程。首先，包括这两篇论文，以及剑桥大学教材<sup class="refplus-num"><a href="#ref-Applied-SDE">[5]</a></sup>，都将任务场景描述为了以下的“连续-离散”系统（Continuous-Discrete System），意思是观测值$y(k)$是离散的、不随时间连续的，而系统的状态$x(t)$是在时间上连续、且带有随机性质的。根据式子（7），引入观测值与内在状态的关系：</p>
<p>$$<br>\begin{align}<br>&amp; dx(t) = f(x(t), t)dt + L(x(t), t)d\beta(t),  \nonumber \\<br>&amp; y(k) = h(x(t_k)) + r_k<br>\end{align}<br>$$</p>
<p>其中$r_k \sim \mathcal{N}(0, R)$为观测时的底噪，$h$是状态$x(t_k)$到观测$y(k)$的映射函数。<span style="text-decoration: underline;"><strong>实际上，这就是状态估计的基本任务设置。</strong></span>这里省略一个证明，就是线性随机微分方程（漂移函数$f$是线性的）的解$x(t)$是一个高斯过程。对应线性SDE写作：</p>
<p>$$<br>\begin{align}<br>d\tilde{x}<br>&amp; = \tilde{f}(x(t), t)dt + L(t)d\beta(t), \nonumber \\<br>&amp; = [-A(t)\tilde{x}(t)+b(t)]dt + L(t)d\beta(t) \\<br>\end{align}<br>$$</p>
<p>其中，线性参数$A(t)$和$b(t)$是待求解项。为了简化书写，省略式子（9）扩散函数$L$中的$x(t)$。根据教材<sup class="refplus-num"><a href="#ref-Applied-SDE">[5]</a></sup>，这个解的边际分布（Marginal Distribution，只包含一部分变量的分布，例如2D高斯分布在其中一个1D轴上的投影是1D高斯分布）是$q(x(t))=\mathcal{N}(x(t)|m(t), P(t))$，其中$m(t)$和$P(t)$分别是$x(t)$的期望和标准差，且应当服从如下的微分方程约束（教材原文是矩阵形式的）：</p>
<p>$$<br>\begin{align}<br>&amp; \frac{d\mathbf{m}(t)}{dt} = -\mathbf{A}(t)\mathbf{m}(t) + \mathbf{b}(t) \\<br>&amp; \frac{d\mathbf{P}(t)}{dt} = -\mathbf{A}(t)\mathbf{P}(t) - \mathbf{P}(t)^\top\mathbf{A}(t) + \mathbf{L}(t)\mathbf{Q}\mathbf{L}(t)^\top<br>\end{align}<br>$$</p>
<p>由于不是重点，这里简要介绍之后的步骤。在理想情况下，边际分布$q(x(t))$应当与对应的真实分布$p(x(t)|y(t))$一致，也就是计算KL散度$\mathrm{KL}(q || p)$。使用拉格朗日乘子法（Lagrange Multipler Function），最小化KL散度公式，并使用两个拉格朗日乘子$\mathbf{\lambda}(t)$、$\mathbf{\Psi}(t)$联立约束等式（10）和（11），求解后得到待求解的线性参数$A(t)$和$b(t)$表达式：</p>
<p>$$<br>\begin{align}<br>&amp; \mathbf{A}(t) = -\mathbb{E}_q[f(\mathbf{x}(t), t)] + 2\mathbf{L}(t)\mathbf{Q}\mathbf{L}^\top(t)\mathbf{\Psi}(t) \nonumber \\<br>&amp; \mathbf{b}(t) = \mathbb{E}_q[f(\mathbf{x}(t), t)] + \mathbf{A}(t)\mathbf{m}(t) + \mathbf{L}(t)\mathbf{Q}\mathbf{L}^\top(t)\mathbf{\lambda}(t) \nonumber \\<br>\end{align}<br>$$</p>
<p>具体过程参见教材<sup class="refplus-num"><a href="#ref-Applied-SDE">[5]</a></sup>的第12.8章节。总而言之，通过假定（其实就是近似）SDE中的未知漂移函数$f(\cdot)$为线性、同时根据线性SDE的解$x(t)$是高斯过程的性质，得到其边际分布$q(x(t))$均值方差的约束式子（10）和（11），联立边际分布和真实分布的KL散度以及这些约束式子，用拉格朗日乘子法计算出$A(t)$和$b(t)$，代入式子（9）得到近似真实SDE的线性SDE。</p>
<h2 id="在状态估计中的应用-Application-in-State-Estimation"><a href="#在状态估计中的应用-Application-in-State-Estimation" class="headerlink" title="在状态估计中的应用 Application in State Estimation"></a>在状态估计中的应用 Application in State Estimation</h2><p>这里以Kevin Course和Prasanth B. Nair的2023年Nature论文<sup class="refplus-num"><a href="#ref-Nature-State">[6]</a></sup>为例。首先，他们分别定义了观测值$y(t)$和隐式状态$x(t)$的映射函数、以及先验和后验两个SDE：</p>
<p>$$<br>\begin{align}<br>&amp; y(t) = h(x(t)) + r(t) \\<br>&amp; dx(t) = f_\theta(x(t), t)dt + \Sigma_\theta(t)d\beta(t) \\<br>&amp; dx(t) =  [-A_\phi(t)x(t)+b_\phi(t)]dt + \Sigma_\theta(t)d\beta(t) \\<br>\end{align}<br>$$</p>
<p>其中，式子（13）是先验SDE，也就是我们待求的SDE；式子（14）是后验SDE，就是通过上一章节的高斯过程近似方法，得到的具体$A_\phi(t)$和$b_\phi(t)$。可以认为，后验SDE是一个用线性SDE近似得来的额外参考，相比直接让一个黑盒的Neural SDE用参数$\theta$去拟合数据，更加地符合SDE的基本规律（<span style="text-decoration: underline;"><strong>也就是说，至少不能比线性SDE的结果还差</strong></span>）。</p>
<p>由此，文章提出的学习目标ELBO（证据下界，Evidence Lower Bound）是：</p>
<p>$$<br>\begin{equation}<br>\begin{split}<br>    &amp; \log p(\mathcal{D}) \\<br>    &amp; = \log \mathbb{E}_{p(\theta)}\left[\mathbb{E}_{\tilde{P}|\theta}\left[\prod_{i=1}^\mathrm{N} p(y(t_i) | x(t_i))\right]\right] \\<br>    &amp;\geq \sum_{i=1}^{N} \mathbb{E}_{p_\phi(x(t_i))}[\log p(y(t_i) | x(t_i))] \\<br>    &amp;-\frac{1}{2}\int_{t_1^{(j)}}^{t_1^{(j+1)}} \mathbb{E}_{q_\phi(x(t))q_\phi(\theta)}{\lvert\lvert{r(x(t), \theta,\phi)}\rvert\rvert_{\Sigma_\theta Q\Sigma^\top_\theta}^2}<br>    \,dt \\<br>    &amp; - D_{KL}(q_\phi(\theta) \mid \mid p(\theta)) = \mathrm{ELBO}(\phi)<br>\end{split}<br>\end{equation}<br>$$</p>
<p>其中，$\mathbb{E}_{\tilde{P}|\theta}$是待求的先验SDE式子（13）对应的期望，$\mathbb{E}_{p(\theta)}$是参数$\theta$所在空间的概率分布对应的期望（实际上这个期望可以拿掉，因为无法对参数空间采样，或者说近似为当前学到的参数$\theta$，毕竟当前参数既然出现了，概率肯定最大）。 $r(x(t)\theta,\phi) = -A_\phi(t)x(t) + b_\phi(t) - f_\theta(x(t), t)$，也就是近似后验SDE约束待求先验SDE，而且只有漂移函数项约束。同时也用到了后面的扩散函数项$\Sigma_\theta$，也就是$\lvert\lvert{v}\rvert\rvert_{\Sigma_\theta Q\Sigma^\top_\theta}^2=v^\top(\Sigma_\theta Q\Sigma^\top_\theta)^{-1}v$。最后一项是对参数分布的约束，$p(\phi)$为给定的先验约束（一般是高斯分布），$q_\phi(\theta)$为真实的参数分布。</p>
<p>实际上，在式子（15）的不等式中，只有不等号右边的第一、二项是真的下界，用到了琴生不等式（Jensen’s Inequation），$\mathbb{E}[-\log(p)] \geq -\log\mathbb{E}(p)$，应该是参考了陈天琦团队的2020年AISTATS会议论文“Scalable gradients for stochastic differential equations”<sup class="refplus-num"><a href="#ref-AISTATS-grad">[8]</a></sup>，他们也定义了先验、近似后验两个SDE，然后对ELBO式子做了分解，得到了一个重构真实值项和一个近似后验SDE约束先验SDE的积分项，也就是第一、二项。</p>
<p>所以，在式子（15）中，KL散度是后来加上的，所以和之前的<a href="https://lmy98129.github.io/2023/12/30/Notes-About-Neural-ODE-and-Beyond-1/#%E5%9C%A8VAE%E4%B8%AD%E7%9A%84%E5%8F%98%E4%BD%93-Variant-in-VAE">变分推断笔记</a>有一定的区别，那边的KL散度是推导出来的，用来约束隐式特征，但是这边的KL散度是约束的参数。只是形式类似，都是重构真实值和KL散度约束的组合。</p>
<h2 id="重参数技巧和均摊分析-Reparameterization-and-Amortization"><a href="#重参数技巧和均摊分析-Reparameterization-and-Amortization" class="headerlink" title="重参数技巧和均摊分析 Reparameterization and Amortization"></a>重参数技巧和均摊分析 Reparameterization and Amortization</h2><p>与之前的<a href="https://lmy98129.github.io/2023/12/30/Notes-About-Neural-ODE-and-Beyond-1/#%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96%E6%8A%80%E5%B7%A7-Reparameterization">变分推断笔记</a>类似，也是为了避免多次前向，那边是给中间的隐式特征分布从$\mathcal{N}(\mu, \sigma)$转为$\mathcal{N}(0, I)$，这里是对隐式特征分布$q_\phi(x(t))$。因为，这是近似后验SDE（参数为$\phi$）输出的当前时刻$t$的隐式状态$x(t)$概率分布，这个概率分布是很难精确找到的，毕竟SDE是带随机性的。当然，也可以通过反复前向多次、采多个$x(t)$的方式来近似它。</p>
<p>但是，可以回想第3章节“高斯过程近似”里面，为了求这个线性SDE的式子（10）和（11），我们已经推导出了$x(t)$的边际分布，也就是$q(x(t))=\mathcal{N}(x(t)|m(t), P(t))$，所以，我们可以直接用这个高斯分布，代替多次前向采样得到的$q_\phi(x(t))$。但是，就算不采样$x(t)$了，式子（15）对应的$r(\cdot)$函数里，每个时刻对应的$A_\phi(t)$和$b_\phi(t)$还是照样得算的。</p>
<p>此时，可以看到式子（10）和（11），刚好就是$A(t)$和$b(t)$构成的，由此可以重新整理成$A_\phi(t)$和$b_\phi(t)$的表达式，然后代入$r(\cdot)$函数。其中，$\oplus$是克罗内克加法（Kronecker Addition）。具体细节可以参见Nature原文<sup class="refplus-num"><a href="#ref-Nature-State">[6]</a></sup>：</p>
<p>$$<br>\begin{align}<br>&amp; r(x(t), t, \theta, \phi) \nonumber \\<br>&amp; = \mathrm{vec}^{-1}((P_\phi(t) \oplus P_\phi(t) )^{-1} \nonumber \\<br>&amp; \mathrm{vec}(\Sigma_\theta Q \Sigma_\theta^\top - \frac{dP_\phi(t)}{dt}))(m_\phi(t) - x(t)) \nonumber \\<br>&amp; + \frac{dm_\phi}{dt} - f_\theta(x(t), t) \\<br>\end{align}<br>$$</p>
<p>另外，<a href="https://lmy98129.github.io/2023/12/30/Notes-About-Neural-ODE-and-Beyond-1/#%E7%A4%BA%E4%BE%8B-Example">之前的笔记</a>里面有简要地提到均摊分析（Amortized Analysis）的概念，当时是为了强调参数量不应当与数据规模挂钩，否则数据量无限大则参数无限大，无法实际应用。Kevin Course和Prasanth B. Nair在NeurIPS 2023上发表的论文“Amortized Reparametrization: Efficient and Scalable Variational Inference for Latent SDEs”<sup class="refplus-num"><a href="#ref-NeurIPS-State">[7]</a></sup>，在标题里面就同时提到了重参数技巧和均摊分析。在这篇文章里，重参数技巧和Nature版本论文一致，均摊分析指的是，尝试将整段长度为N的时间序列数据，切分为M份，每份长度为N/M，从而更好地实现并行化的训练和预测。</p>
<h2 id="结语-Conclusions"><a href="#结语-Conclusions" class="headerlink" title="结语 Conclusions"></a>结语 Conclusions</h2><p>至此，已经写好了三篇笔记，基本搞懂了Neural ODE、SDE以及状态估计。但是，还有很多的细节没有搞懂，例如这篇里面省略的若干推导过程，以及状态估计的两篇论文<sup class="refplus-num"><a href="#ref-Nature-State">[6]</a></sup><sup class="refplus-num"><a href="#ref-NeurIPS-State">[7]</a></sup>的更多应用场景、数据和建模方式，尤其是NeurIPS论文的附加材料<sup class="refplus-num"><a href="#ref-NeurIPS-State">[7]</a></sup>给出了针对不同任务的网络结构设计。此外，现在的笔记内容全是个人理解，难免错漏，之后肯定要修改。</p>
<p>令人意外的是，三篇笔记前后穿插了很多知识点，ODE和SDE的微分和积分形式、变分推断中的ELBO、重参数技巧、均摊分析。回想我在<a href="https://lmy98129.github.io/2023/12/30/Notes-About-Neural-ODE-and-Beyond-1/#more">第一篇笔记</a>里吐槽的“各种统计学、数据科学、机器学习的视频都在强调”，就体现了统计机器学习领域各理论分支的共通之处。希望这次的笔记能够成为一个新的契机，让我们继续探索，看懂更多精彩的东西！</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul id="refplus"><li id="ref-torchdiffeq" data-num="1">[1]  Chen R T Q. https://github.com/rtqichen/torchdiffeq</li><li id="ref-Neural-ODE" data-num="2">[2]  Chen R T Q, Rubanova Y, Bettencourt J, et al. Neural ordinary differential equations[J]. Advances in neural information processing systems, 2018, 31.</li><li id="ref-Latent-ODE" data-num="3">[3]  Rubanova Y, Chen R T Q, Duvenaud D K. Latent ordinary differential equations for irregularly-sampled time series[J]. Advances in neural information processing systems, 2019, 32.</li><li id="ref-Neural-SDE" data-num="4">[4]  Tzen B, Raginsky M. Neural stochastic differential equations: Deep latent gaussian models in the diffusion limit[J]. arXiv preprint arXiv:1905.09883, 2019.</li><li id="ref-Applied-SDE" data-num="5">[5]  Särkkä S, Solin A. Applied stochastic differential equations[M]. Cambridge University Press, 2019.</li><li id="ref-Nature-State" data-num="6">[6]  Course K, Nair P B. State estimation of a physical system with unknown governing equations[J]. Nature, 2023, 622(7982): 261-267.</li><li id="ref-NeurIPS-State" data-num="7">[7]  Course K, Nair P B. Amortized Reparametrization: Efficient and Scalable Variational Inference for Latent SDEs[C]//Thirty-seventh Conference on Neural Information Processing Systems. 2023.</li><li id="ref-AISTATS-grad" data-num="8">[8]  Li X, Wong T K L, Chen R T Q, et al. Scalable gradients for stochastic differential equations[C]//International Conference on Artificial Intelligence and Statistics. PMLR, 2020: 3870-3882.</li></ul>
<p><br></p>
<blockquote>
<p>感谢阅读！如有意见和建议，欢迎通过首页的联系方式联系作者。<br>本文参考资料均来源于网络，作者保留相关权利，转载请注明出处。</p>
</blockquote>

    <style>
    #refplus, #refplus li{ 
        padding:0;
        margin:0;
        list-style:none;
    }；
    </style>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>
    <script>
    document.querySelectorAll(".refplus-num").forEach((ref) => {
        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');
        let refel = document.querySelector(refid);
        let refnum = refel.dataset.num;
        let ref_content = refel.innerText.replace(`[${refnum}]`,'');
        tippy(ref, {
            content: ref_content,
        });
    });
    </script>
    
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/experience/" rel="tag"><i class="fa fa-tag"></i> experience</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/" rel="next" title="Notes About Neural ODE and Beyond 2">
                <i class="fa fa-chevron-left"></i> Notes About Neural ODE and Beyond 2
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Mengyin Liu" />
            
              <p class="site-author-name" itemprop="name">Mengyin Liu</p>
              <p class="site-description motion-element" itemprop="description">Stay Hungry, stay foolish</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/%7C%7C%20archive">
                
                    <span class="site-state-item-count">10</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/lmy98129" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:blean@live.cn" target="_blank" title="Mail"><i class="fa fa-fw fa-envelope"></i>Mail</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://scholar.google.com/citations?user=hN7koAYAAAAJ&hl=zh-CN" target="_blank" title="Scholar"><i class="fa fa-fw fa-google"></i>Scholar</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/liu-meng-yin-68/" target="_blank" title="知乎"><i class="fa fa-fw fa-comment"></i>知乎</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8B%98%E8%AF%AF%E5%92%8C%E4%B8%80%E4%BA%9B%E6%8E%A8%E8%AE%BA-Corrigendum-and-Corollaries"><span class="nav-number">1.</span> <span class="nav-text">勘误和一些推论 Corrigendum and Corollaries</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B-Stochastic-Differential-Equation"><span class="nav-number">2.</span> <span class="nav-text">随机微分方程 Stochastic Differential Equation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B%E8%BF%91%E4%BC%BC-Gaussian-Process-Approximation-of-SDE-Solutions"><span class="nav-number">3.</span> <span class="nav-text">高斯过程近似 Gaussian Process Approximation of SDE Solutions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9C%A8%E7%8A%B6%E6%80%81%E4%BC%B0%E8%AE%A1%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8-Application-in-State-Estimation"><span class="nav-number">4.</span> <span class="nav-text">在状态估计中的应用 Application in State Estimation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%87%8D%E5%8F%82%E6%95%B0%E6%8A%80%E5%B7%A7%E5%92%8C%E5%9D%87%E6%91%8A%E5%88%86%E6%9E%90-Reparameterization-and-Amortization"><span class="nav-number">5.</span> <span class="nav-text">重参数技巧和均摊分析 Reparameterization and Amortization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%93%E8%AF%AD-Conclusions"><span class="nav-number">6.</span> <span class="nav-text">结语 Conclusions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">7.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mengyin Liu</span>

  

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>








<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共69.7k字</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="总访客量">
      <i class="fa fa-user"></i>
      本站访客数<span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="总访问量">
      <i class="fa fa-eye"></i>
      总访问量<span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  















  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.0.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.0.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.0.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.0.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.4"></script>



  



	





  





  










  





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  

  
  <script type="text/javascript" src="/js/src/exturl.js?v=6.0.4"></script>


  

</body>
</html>
