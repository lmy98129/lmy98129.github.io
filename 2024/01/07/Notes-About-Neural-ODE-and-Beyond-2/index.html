<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">











<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.0.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.4">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.4" color="#222">







<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Gemini',
    version: '6.0.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  
  <meta name="keywords" content="experience," />


<meta name="description" content="本文是学习Neural ODE过程中的第二篇个人笔记 主要是ODE的基本原理，包括前向计算过程，以及基于伴随方法的反向传播同时会简要地提一下State Estimation状态估计，其余论文细节以及SDE会在下一次记录。">
<meta property="og:type" content="article">
<meta property="og:title" content="Notes About Neural ODE and Beyond 2">
<meta property="og:url" content="http://lmy98129.github.io/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/index.html">
<meta property="og:site_name" content="NeXT">
<meta property="og:description" content="本文是学习Neural ODE过程中的第二篇个人笔记 主要是ODE的基本原理，包括前向计算过程，以及基于伴随方法的反向传播同时会简要地提一下State Estimation状态估计，其余论文细节以及SDE会在下一次记录。">
<meta property="og:locale">
<meta property="og:image" content="http://lmy98129.github.io/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/eulerdiagram.png">
<meta property="og:image" content="http://lmy98129.github.io/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/euler-method.png">
<meta property="og:image" content="http://lmy98129.github.io/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/ResBlock.png">
<meta property="og:image" content="http://lmy98129.github.io/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/resnet-vs-neuralode.png">
<meta property="og:image" content="http://lmy98129.github.io/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/adjoint-method.png">
<meta property="og:image" content="http://lmy98129.github.io/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/reverse-mode.png">
<meta property="og:image" content="http://lmy98129.github.io/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/state-estimation.png">
<meta property="article:published_time" content="2024-01-07T09:15:51.000Z">
<meta property="article:modified_time" content="2024-01-08T14:19:52.922Z">
<meta property="article:author" content="Mengyin Liu">
<meta property="article:tag" content="experience">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://lmy98129.github.io/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/eulerdiagram.png">






  <link rel="canonical" href="http://lmy98129.github.io/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>
  <title>Notes About Neural ODE and Beyond 2 | NeXT</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

<meta name="generator" content="Hexo 5.4.2"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">NeXT</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
        </li>
      
        
        <li class="menu-item menu-item-academic">
          <a href="/academic/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-flask"></i> <br />学术</a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />标签</a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>
        </li>
      

      
    </ul>
  

  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://lmy98129.github.io/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mengyin Liu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NeXT">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Notes About Neural ODE and Beyond 2</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2024-01-07T17:15:51+08:00">2024-01-07</time>
            

            
            
              
                
              
            

            
              
              <span class="post-meta-divider">|</span>
              

              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2024-01-08T22:19:52+08:00">2024-01-08</time>
            
          </span>

          

          
            
          

          
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文是学习Neural ODE过程中的第二篇个人笔记</p>
<p>主要是ODE的基本原理，包括前向计算过程，以及基于伴随方法的反向传播<br>同时会简要地提一下State Estimation状态估计，其余论文细节以及SDE会在下一次记录。</p>
<span id="more"></span>
<h2 id="常微分方程-Ordinary-Differential-Equation"><a href="#常微分方程-Ordinary-Differential-Equation" class="headerlink" title="常微分方程 Ordinary Differential Equation"></a>常微分方程 Ordinary Differential Equation</h2><p>常微分方程指的是自变量只有一个的微分方程，在这里一般表示的是时间$t$，若有多个自变量则为偏微分方程。设因变量为$y$，两者的变化关系$f(y, t)$可以用常微分方程表示：</p>
<p>$$<br>\begin{equation}<br>dy = f(y, t)dt<br>\end{equation}<br>$$</p>
<p>为了得到某一个时刻$\mathrm{T}$上的$y_\mathrm{T}$具体值, 对式子（1）在$[0, \mathrm{T}]$上求定积分：</p>
<p>$$<br>\begin{equation}<br>y_\mathrm{T} = y_0 + \int_0^\mathrm{T} f(y, t)dt<br>\end{equation}<br>$$</p>
<p>其中，$y_0$为$y$在$t=0$时刻的初始值。显然，如果没有这个初始值$y_0$，式子（2）的积分项只是起止时刻的$y$的总变化量，也就是$\Delta y=y_\mathrm{T}-y_0$。因此，可以称这种问题为初值问题（IVP，Initial Value Problem）。</p>
<h2 id="常微分方程的求解器-ODE-Solver"><a href="#常微分方程的求解器-ODE-Solver" class="headerlink" title="常微分方程的求解器 ODE Solver"></a>常微分方程的求解器 ODE Solver</h2><p>对于$f(y,t)$，如果它是一些简单的函数，则可以直接求解得到积分的解析解。然而，当它过于复杂而无法求解积分时，需要用数值计算的方法来近似计算其积分值，记为求解器$\mathrm{ODESolver}$，则式子（2）可以改写为：</p>
<p>$$<br>y_\mathrm{T} = \mathrm{ODESolver}(y_0, f, [0, \mathrm{T}])<br>$$</p>
<p>最简单的$\mathrm{ODESolver}$是欧拉方法（Euler’s Method）。设在自变量轴$t$上的固定步长$h$，从初始时间$t_0=0$到下一步长时间$t_1=t_0+h$段内，为了知道$t_1$上具体的$y$值是多少，需要和式子（2）一样求积分，但是过于复杂没办法求解析解。</p>
<p>由于$f(y, t)$表示的是$y$随$t$的变化量，也就是导数，则可以求出在初始时间$t_0$对应的初值$y_0$上的导数$f(y_0, t_0)$。那么，根据导数的性质，将该导数作为斜率（Slope），乘以自变量轴上$t$的变化量，也就是步长$h$，可以得到因变量轴$y$上经过该步长的变化量$h \ f(y_0, t_0)$。将这个变化量加上初值，则为下一步长时间的因变量值$y_1$（的近似值）：</p>
<p>$$<br>y_1 = y_0 + h \ f(y_0, t_0)<br>$$</p>
<p><img src="/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/eulerdiagram.png" alt="eulerdiagram"></p>
<center>欧拉方法示意图，图中的自变量轴写成了$x$，而不是$t$。<a target="_blank" rel="noopener" href="https://teaching.smp.uq.edu.au/scims/Appl_analysis/Eulers_method.html">图片</a>来自昆士兰大学<sup class="refplus-num"><a href="#ref-Queensland">[1]</a></sup></center>

<p><br></p>
<p>在得到了下一对自变量和因变量$(t_1, y_1)^\top$后，继续取$f(y_1, t_1)$计算$y_2$。如此迭代，则可以算到$y_\mathrm{T}$，但是该解析解一定会和真实值有一定的误差，因为这种计算方法基于步长$h$和斜率$f(x_t, y_t)$构成的三角形，而不是平滑变化过程。</p>
<p><img src="/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/euler-method.png" alt="euler-method"></p>
<center>欧拉方法计算过程的轨迹与真实函数的对比。<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Euler_method">图片</a>来自维基百科<sup class="refplus-num"><a href="#ref-Wikipedia">[2]</a></sup></center>

<p><br></p>
<p>但是，随着步长$h$的逐渐减小，欧拉方法计算过程的轨迹也会和真实函数越来越近，可以参见维基百科<sup class="refplus-num"><a href="#ref-Wikipedia">[2]</a></sup>给出的示例。值得注意的是，<strong>这一计算过程不仅能够得到$y_\mathrm{T}$本身，也能将过程中若干个中间结果$y_t$连起来，可视化$y$的变化过程</strong>。</p>
<h2 id="神经常微分方程-Neural-ODE"><a href="#神经常微分方程-Neural-ODE" class="headerlink" title="神经常微分方程 Neural ODE"></a>神经常微分方程 Neural ODE</h2><p>回顾上一节，常微分方程求解器的前提条件是，已知变化关系函数$f(y,t)$的具体形式，且$f(y,t)$的积分难求解析解。但在实际应用中，还有更多只知道若干个$(t, y_t)^\top$的数据点（可能均匀采样，也可能不均采样），不知道$f(y,t)$具体形式的情况。那么，如何通过学习的方式，拟合出$f(y,t)$后，再使用$\mathrm{ODESolver}$求解$y_\mathrm{T}$呢？</p>
<p>在讲2018年理论三大会NeurIPS的Best Paper、陈天琦团队发表的Neural ODE<sup class="refplus-num"><a href="#ref-Neural-ODE">[3]</a></sup>之前，我们需要回顾一个更加熟悉的工作，也就是已经成为CV界著名的基础设施、大佬何凯明团队在CV三大会CVPR 2016上发表的ResNet<sup class="refplus-num"><a href="#ref-ResNet">[4]</a></sup>。</p>
<p><img src="/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/ResBlock.png" alt="ResBlock"></p>
<center>ResNet中的一个Residual Block，图中的隐式特征为$x$。</center>

<p><br></p>
<p>对于ResNet中的第$t$个Residual Block，其输入的隐式特征为$x_t$，处理该特征的网络参数为$\theta_t$，输出的隐式特征为$x_{t+1}$，则该Residual Block输入输出隐式特征的过程，可以用公式表示为：</p>
<p>$$<br>x_{t+1} = x_t + f(x_t, \theta_t)<br>$$</p>
<p>由于Resisual Block的个数一般是整数个，则步长$h=1$。当步长大小趋近于0，也即$h \rightarrow 0$时，有：</p>
<p>$$<br>\begin{align}<br>&amp; x_{t+1} = x_t + f(x_t, \theta_t) \nonumber \\<br>&amp; \Rightarrow \frac{x_{t+1} - x_t}{1} = f(x_t, \theta_t) \nonumber \\<br>&amp; \Rightarrow \frac{x_{t+h} - x_t}{h} = f(x_t, \theta_t) \nonumber \\<br>&amp; \Rightarrow \lim_{h \rightarrow 0} \frac{x_{t+h} - x_t}{h} = f(x_t, t, \theta) \nonumber \\<br>&amp; \Rightarrow \frac{dx_t}{dt} = f(x_t, t, \theta) \\<br>\end{align}<br>$$</p>
<p>其中，在转换为极限时，Residual Block $f(x_t, \theta_t)$需要改成$f(x_t, t, \theta)$。因为当步长$h \rightarrow 0$时，需要无数个$\theta_t$进行迭代，参数总量趋于无穷，无法实际应用。转为固定参数$\theta$后，$t$可以提示网络要输出对应时间下的结果。</p>
<p>所以，如式子（3）所示，ResNet作为一个神经网络，同样可以表示为常微分方程ODE的形式。当然，也可以从式子（3）中对步长（从步长为固定整数1、迭代次数也是固定的，到步长趋于0、迭代次数趋于无穷）、网络参数（从每次迭代使用不同的网络参数，到所有迭代共享一个网络参数且用$t$提示网络当前迭代时间）的修改看出，原始的ResNet并不能表示常微分方程，需要进行一定的改造。</p>
<p><img src="/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/resnet-vs-neuralode.png" alt="resnet-vs-neuralode"></p>
<center>ResNet与Neural ODE的对比。图片来自Neural ODE论文原文<sup class="refplus-num"><a href="#ref-Neural-ODE">[3]</a></sup></center>

<p><br></p>
<p>在Neural ODE原文<sup class="refplus-num"><a href="#ref-Neural-ODE">[3]</a></sup>的首图中，可以更直观地看出两者的区别。纵轴为网络深度，可以视为自变量轴$t$；横轴为输入、输出和隐式特征，可以视为因变量$x$或$y$。左图是ResNet，时间$t$的步长为固定整数1，每一次$x$都只能从上一个步长变化到下一个，变化过程是离散的，只能拿这些因变量作为验证点（图中黑色实心圆点，evaluation locations）；右图图是Neural ODE，时间$t$的步长为无穷小，可以近似为连续过程，任意不均匀的时间点都可以计算$x$，作为验证点。</p>
<p>同样的，对于循环神经网络RNN，虽然与ResNet的网络参数使用方式不同，所有迭代共享一个网络参数，但也无法迭代非整数个步长。上图没有考虑网络参数是否共享的问题，因此RNN也一样只能输出类似于左侧的结果。</p>
<p>因此，Neural ODE能够直接在不知道$f(y,t)$具体形式的情况，基于若干个$(t, y_t)^\top$的数据点作为训练集，用网络参数$\theta$拟合出一个神经网络形式的ODE，也就是$f(y_t, t, \theta)$，然后再将这一形式代入到$\mathrm{ODESolver}$中，求解出需要的$y_\mathrm{T}$值。</p>
<h2 id="伴随方法-Adjoint-Method"><a href="#伴随方法-Adjoint-Method" class="headerlink" title="伴随方法 Adjoint Method"></a>伴随方法 Adjoint Method</h2><p>那么，如何训练出一个Neural ODE呢？最自然的方式，肯定是在某一点有真实值$(t_1, y_t)^\top$时，将其与Neural ODE的输出$\hat{y_t}$计算损失并反向传播梯度。但是，从上述Neural ODE的性质可以看出，它的迭代步长远小于ResNet、RNN的整数步长，前向输出$\hat{y_t}$所经历的迭代次数也非常多（例如$10^2\sim 10^3$次）。如果直接建立梯度的计算图，将需要保存大量的前向和反向的中间特征和中间梯度，消耗大量资源。因此，Neural ODE原文采用了伴随方法（Adjoint Method）。这里参考这篇知乎文章的推导过程<sup class="refplus-num"><a href="#ref-Zhihu-Adjoint">[5]</a></sup>。</p>
<p>首先，需要注意的是，Neural ODE并未直接对预测值$\hat{y_t}$使用式子（3）进行ODE方式的更新，而是和ResNet、RNN一样，对隐式特征进行更新。然后通过另一个神经网络，输入隐式特征，输出预测值。这里为了和原文保持一致，将隐式特征记作$z(t)$。省略输出预测值、预测值与真实值计算损失的过程，将损失函数$\mathcal{L}$记为隐式特征的函数$\mathcal{L}(z(t))$：</p>
<p>$$<br>\begin{align}<br>\mathcal{L}(z(t_1)) &amp; = \mathcal{L}(\mathrm{ODESolver}(z(t_0), f, t_0, t_1, \theta)) \nonumber \\<br>&amp; = \mathcal{L}(z(t_0) + \int_{t_0}^{t_1} f(z(t), t, \theta)dt) \nonumber \\<br>\end{align}<br>$$</p>
<p>由链式法则，在任意时刻$t \leq t_1$（因为是反向传播，从最终的$t_1$算梯度往回传），对于损失函数求隐式特征偏导：</p>
<p>$$<br>\frac{\partial \mathcal{L}}{\partial z(t)} = \frac{\partial \mathcal{L}}{\partial z(t_1)} \frac{\partial z(t_1)}{\partial z(t)}<br>$$</p>
<p>定义一个伴随状态（Adjoint State），与时间$t$相关的函数$a(t)$：</p>
<p>$$<br>\begin{equation}<br>a(t) = \frac{\partial \mathcal{L}}{\partial z(t)}<br>\end{equation}<br>$$</p>
<p>那么对于任意$t+\epsilon &gt; t$时刻（包括$t_1$在内），都有：</p>
<p>$$<br>\begin{align}<br>\frac{\partial \mathcal{L}}{\partial z(t)} &amp; = \frac{\partial \mathcal{L}}{\partial z(t+\epsilon)} \frac{\partial z(t+\epsilon)}{\partial z(t)} \nonumber \\<br>&amp; = a(t+\epsilon) \frac{\partial z(t+\epsilon)}{\partial z(t)} \\<br>\end{align}<br>$$</p>
<p>从$t$时刻开始，求解$t+\epsilon$时刻的隐式特征$z(t+\epsilon)$，可以算积分：</p>
<p>$$<br>\begin{equation}<br>z(t+\epsilon) = z(t) + \int_{t}^{t+\epsilon} f(z(t’), t’, \theta)dt’<br>\end{equation}<br>$$</p>
<p>将式子（4）、（6）代入式子（5），可以得到：</p>
<p>$$<br>\begin{align}<br>&amp; a(t) = a(t+\epsilon) \frac{\partial}{\partial z(t)} ( z(t) + \int_{t}^{t+\epsilon} f(z(t’), t’, \theta)dt’) \nonumber \\<br>&amp; = a(t+\epsilon) (1 + \frac{\partial}{\partial z(t)} (\int_{t}^{t+\epsilon} f(z(t’), t’, \theta)dt’)) \nonumber \\<br>&amp; \Rightarrow a(t+\epsilon) - a(t) \nonumber \\<br>&amp; = - a(t+\epsilon) \frac{\partial}{\partial z(t)} (\int_{t}^{t+\epsilon} f(z(t’), t’, \theta)dt’)<br>\end{align}<br>$$</p>
<p>式子（7）即可描述伴随方程$a(t)$随时间$t$的变化，进一步计算$\frac{da(t)}{dt}$：</p>
<p>$$<br>\begin{align}<br>&amp; \frac{da(t)}{dt} = \lim_{\epsilon \rightarrow 0} \frac{a(t+\epsilon) - a(t)}{\epsilon} \nonumber \\<br>&amp; = \lim_{\epsilon \rightarrow 0} \frac{- a(t+\epsilon) \frac{\partial}{\partial z(t)} (\int_{t}^{t+\epsilon} f(z(t’), t’, \theta)dt’)}{\epsilon} \nonumber \\<br>&amp; = \lim_{\epsilon \rightarrow 0} \frac{- a(t+\epsilon) \frac{\partial}{\partial z(t)} (\epsilon \cdot f(z(t), t, \theta))}{\epsilon} \nonumber \\<br>&amp; = \lim_{\epsilon \rightarrow 0} - a(t+\epsilon) \frac{\partial f(z(t), t, \theta)}{\partial z(t)} \nonumber \\<br>&amp; = - a(t) \frac{\partial f(z(t), t, \theta)}{\partial z(t)} \\<br>\end{align}<br>$$</p>
<p>由于$\epsilon \rightarrow 0$，且偏导$\frac{\partial}{\partial z(t)}$与$\epsilon$无关。一个小区间的积分，就是高度为函数值，宽度为无穷小的矩形面积，则上式中的积分，就等于在$t$这个点上的$f$函数值乘以一个$dt’ = \epsilon$。分子分母两个$\epsilon$约去后，又由$\epsilon \rightarrow 0$，则$a(t+\epsilon) = a(t)$。</p>
<p>式子（8）就是伴随状态$a(t)=\frac{\partial \mathcal{L}}{\partial z(t)}$随时间的常微分方程，既然是常微分方程，就可以使用$\mathrm{ODESolver}$近似求解任意时刻的伴随状态，无需对偏导$\frac{\partial \mathcal{L}}{\partial z(t)}$本身建立复杂的反向传播计算图：</p>
<p>$$<br>\begin{align}<br>&amp; a(t_0) = a(t_1) + \int_{t_1}^{t_0} \frac{da(t)}{dt} dt \nonumber \\<br>&amp; = a(t_1) - \int_{t_1}^{t_0} a(t) \frac{\partial f(z(t), t, \theta)}{\partial z(t)} dt \nonumber \\<br>&amp; = \mathrm{ODESolver}(a(t_1), - a(t) \frac{\partial f}{\partial z(t)}, t_1, t_0)<br>\end{align}<br>$$</p>
<p>但是，$\frac{\partial \mathcal{L}}{\partial z(t)}$只是反向传播的一部分，计算出这个损失函数与隐式特征的偏导之后，还要计算出损失函数与网络参数的偏导，也定为伴随状态$a_\theta(t) = \frac{\partial \mathcal{L}}{\partial \theta(t)}$。虽然，根据Neural ODE的性质，不同迭代次数下的网络参数$\theta$是不变的。但是，由于最终的梯度由$\geq 1$个不同真实值$(t, y_t)^\top$计算损失累积得到，所以认为仍然与时间有关，但是$\frac{\partial \theta}{\partial t} = 0$。由链式法则：</p>
<p>$$<br>\begin{align}<br>&amp; a_\theta(t) = \frac{\partial \mathcal{L}}{\partial z(t+\epsilon)} \frac{\partial z(t+\epsilon)}{\partial \theta(t)} \nonumber \\<br>&amp; + \frac{\partial \mathcal{L}}{\partial \theta(t+\epsilon)} \frac{\partial \theta(t+\epsilon)}{\theta(t)} \nonumber \\<br>&amp; = a(t+\epsilon) \frac{\partial z(t+\epsilon)}{\partial \theta(t)} + a_\theta (t+\epsilon) \cdot 1 \\<br>&amp; = a(t+\epsilon)  \int_{t}^{t+\epsilon} \frac{\partial f}{\partial \theta} dt + a_\theta (t+\epsilon) \nonumber \\<br>&amp; \Rightarrow a_\theta(t+\epsilon) - a_\theta(t) \nonumber \\<br>&amp; =  - a(t+\epsilon)  \int_{t}^{t+\epsilon} \frac{\partial f}{\partial \theta} dt \\<br>\end{align}<br>$$</p>
<p>其中，式子（6）的$z(t+\epsilon)$积分可以代入到式子（10）中。式子（11）即可描述伴随方程$a_\theta(t)$随时间$t$的变化，进一步计算$\frac{da_\theta(t)}{dt}$：</p>
<p>$$<br>\begin{align}<br>&amp; \frac{da_\theta(t)}{dt} = \lim_{\epsilon \rightarrow 0} \frac{a_\theta(t+\epsilon) - a_\theta(t)}{\epsilon} \nonumber \\<br>&amp; = \lim_{\epsilon \rightarrow 0} \frac{- a(t+\epsilon)  \int_{t}^{t+\epsilon} \frac{\partial f}{\partial \theta} dt}{\epsilon} \nonumber \\<br>&amp; = - a(t) \frac{\partial f(z(t), t, \theta)}{\partial \theta(t)} \\<br>\end{align}<br>$$</p>
<p>过程与式子（8）相同，此处省略若干步骤。由此，式子（12）也是对伴随状态$a_\theta(t) = \frac{\partial \mathcal{L}}{\partial \theta(t)}$的常微分方程，也可以使用$\mathrm{ODESolver}$近似求解任意时刻的伴随状态，无需对$\frac{\partial \mathcal{L}}{\partial \theta(t)}$本身建立复杂的反向传播计算图，同式子（9）：</p>
<p>$$<br>\begin{align}<br>&amp; a_\theta(t_0) = a_\theta(t_1) + \int_{t_1}^{t_0} \frac{da_\theta(t)}{dt} dt \nonumber \\<br>&amp; = a_\theta(t_1) - \int_{t_1}^{t_0} a(t) \frac{\partial f(z(t), t, \theta)}{\partial \theta} dt \nonumber \\<br>&amp; = \mathrm{ODESolver}(a_\theta(t_1), - a(t) \frac{\partial f}{\partial \theta}, t_1, t_0)<br>\end{align}<br>$$</p>
<p>从式子（13）可以看到，由于$a_\theta(t)$依赖于$a(t)$的求解，$a(t)$又依赖于$z(t)$的求解，因此Neural ODE原文将它们同时送入同一个$\mathrm{ODESolver}$，从而进一步提高了计算效率。为了便于复用，Neural ODE的作者团队封装了这套特殊算法的代码库，作为他们开源项目的组成部分：<a target="_blank" rel="noopener" href="https://github.com/rtqichen/torchdiffeq">torchdiffeq</a>。</p>
<p><img src="/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/adjoint-method.png" alt="adjoint-method"></p>
<center>Neural ODE针对伴随方法设计的反向传播算法。<br>其中“aug_dynamics”相当于式子（3）、（9）和（13）的联立之后，互相代入现有值<br> 图片来自Neural ODE论文原文<sup class="refplus-num"><a href="#ref-Neural-ODE">[3]</a></sup></center>

<p><br></p>
<p>有趣的是，可以注意到前面的时间全是$t_0$和$t_1$，而且是从$t_1$倒着回到$t_0$（这倒是好理解，因为是算梯度，肯定是从最后一次计算损失的时候往回传）。为什么不是整条预测序列呢（例如从$t_1$到$t_\mathrm{start}$）？如果这样的话，可以看到用上图中只有一个真实值$(t_1, y_{t_1})^\top$计算损失得到的$\frac{\partial \mathcal{L}}{\partial z(t_1)}$，忽略了从$t_1$到$t_\mathrm{start}$中间若干个其他通过真实值$(t, y_t)^\top$得到的$\frac{\partial \mathcal{L}}{\partial z(t_1)}$，这样会导致最终计算出的$\frac{\partial \mathcal{L}}{\partial z(t_0)}$有较大的误差。因此，Neural ODE原文只用上述算法计算真实值之间的伴随状态，遇到真实值就用真实值的$\frac{\partial \mathcal{L}}{\partial z(t_1)}$作为初始值，重新用上述算法计算，从而使得累积误差最小。</p>
<p><img src="/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/reverse-mode.png" alt="reverse-mode"></p>
<center>Neural ODE使用伴随状态进行反向传播的方式。<br>上图：先计算损失$\mathcal{L}$，得到真实的$\frac{\partial \mathcal{L}}{\partial z(t)}$。下图：红线是$\mathrm{ODESolver}$计算出来的伴随状态，蓝色虚线是基于真实的$\frac{\partial \mathcal{L}}{\partial z(t)}$来重新开始计算$a(t)$，从而使得误差累积更小。图片来自Neural ODE论文原文<sup class="refplus-num"><a href="#ref-Neural-ODE">[3]</a></sup></center>

<h2 id="在状态估计中的应用-Application-in-State-Estimation"><a href="#在状态估计中的应用-Application-in-State-Estimation" class="headerlink" title="在状态估计中的应用 Application in State Estimation"></a>在状态估计中的应用 Application in State Estimation</h2><p>从前文可以看到，Neural ODE的隐式特征（也可以称为隐式状态，Hidden State）实际上和ResNet、RNN一样，是黑盒化的特征向量或张量，实际上无法反应与客观世界对应的、结构化的内在状态，同时也很难直观地可视化。为了能够在一定程度上将这些隐式状态可视化，Kevin Course和Prasanth B. Nair在2023年10月的Nature正刊上发表了“State estimation of a physical system with unknown governing equations”<sup class="refplus-num"><a href="#ref-Nature-State">[6]</a></sup>。</p>
<p><img src="/2024/01/07/Notes-About-Neural-ODE-and-Beyond-2/state-estimation.png" alt="state-estimation"></p>
<center>基于Neural SDE的状态估计结果。左图为学习到的隐式状态$x(t)$，右图为对应流体仿真结果的真实值、预测值和标准差（蓝色，用于衡量预测值与真实值的偏差程度）。图片来自Nature论文原文<sup class="refplus-num"><a href="#ref-Nature-State">[6]</a></sup></center>

<p><br></p>
<p>该工作采用了相比Neural ODE更加贴近真实物理系统、考虑了随机布朗运动的Neural SDE（SDE即随机微分方程，Stochastic Differential Equation），通过在有限的观测数据$y(t)$上学习，得到了更强的对$y(t)$进行内插和外推的能力，同时也学习到了更加可解释的隐式状态$x(t)$。</p>
<p>虽然，他们也认为，Neural SDE相比纯符号模型（通过建模方程、待定参数拟合的方式）可解释性更低，但是也可用于（1）无法用方程拟合的复杂高维数据，例如视频数据<sup class="refplus-num"><a href="#ref-NeurIPS-State">[7]</a></sup>；（2）无法确定合适的基础方程用于建模和拟合的情况。至于这些工作如何实现状态估计的、以及相应的技术细节，就留到下次笔记。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul id="refplus"><li id="ref-Queensland" data-num="1">[1]  Euler's method. https://teaching.smp.uq.edu.au/scims/Appl_analysis/Eulers_method.html</li><li id="ref-Wikipedia" data-num="2">[2]  Euler method. https://en.wikipedia.org/wiki/Euler_method</li><li id="ref-Neural-ODE" data-num="3">[3]  Chen R T Q, Rubanova Y, Bettencourt J, et al. Neural ordinary differential equations[J]. Advances in neural information processing systems, 2018, 31.</li><li id="ref-ResNet" data-num="4">[4]  He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.</li><li id="ref-Zhihu-Adjoint" data-num="5">[5]  Maple小七. 理解伴随法(Adjoint Method)在Neural ODE中的应用. https://zhuanlan.zhihu.com/p/337575425</li><li id="ref-Nature-State" data-num="6">[6]  Course K, Nair P B. State estimation of a physical system with unknown governing equations[J]. Nature, 2023, 622(7982): 261-267.</li><li id="ref-NeurIPS-State" data-num="7">[7]  Course K, Nair P B. Amortized Reparametrization: Efficient and Scalable Variational Inference for Latent SDEs[C]//Thirty-seventh Conference on Neural Information Processing Systems. 2023.</li></ul>
<p><br></p>
<blockquote>
<p>感谢阅读！如有意见和建议，欢迎通过首页的联系方式联系作者。<br>本文参考资料均来源于网络，作者保留相关权利，转载请注明出处。</p>
</blockquote>

    <style>
    #refplus, #refplus li{ 
        padding:0;
        margin:0;
        list-style:none;
    }；
    </style>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>
    <script>
    document.querySelectorAll(".refplus-num").forEach((ref) => {
        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');
        let refel = document.querySelector(refid);
        let refnum = refel.dataset.num;
        let ref_content = refel.innerText.replace(`[${refnum}]`,'');
        tippy(ref, {
            content: ref_content,
        });
    });
    </script>
    
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/experience/" rel="tag"><i class="fa fa-tag"></i> experience</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2023/12/30/Notes-About-Neural-ODE-and-Beyond-1/" rel="next" title="Notes About Neural ODE and Beyond 1">
                <i class="fa fa-chevron-left"></i> Notes About Neural ODE and Beyond 1
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2024/01/10/Notes-About-Neural-ODE-and-Beyond-3/" rel="prev" title="Notes About Neural ODE and Beyond 3">
                Notes About Neural ODE and Beyond 3 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Mengyin Liu" />
            
              <p class="site-author-name" itemprop="name">Mengyin Liu</p>
              <p class="site-description motion-element" itemprop="description">Stay hungry, stay foolish</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">15</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/lmy98129" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:blean@live.cn" target="_blank" title="Mail"><i class="fa fa-fw fa-envelope"></i>Mail</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://scholar.google.com/citations?user=hN7koAYAAAAJ&hl=zh-CN" target="_blank" title="Scholar"><i class="fa fa-fw fa-google"></i>Scholar</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/liu-meng-yin-68/" target="_blank" title="知乎"><i class="fa fa-fw fa-comment"></i>知乎</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B-Ordinary-Differential-Equation"><span class="nav-number">1.</span> <span class="nav-text">常微分方程 Ordinary Differential Equation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E7%9A%84%E6%B1%82%E8%A7%A3%E5%99%A8-ODE-Solver"><span class="nav-number">2.</span> <span class="nav-text">常微分方程的求解器 ODE Solver</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B-Neural-ODE"><span class="nav-number">3.</span> <span class="nav-text">神经常微分方程 Neural ODE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%B4%E9%9A%8F%E6%96%B9%E6%B3%95-Adjoint-Method"><span class="nav-number">4.</span> <span class="nav-text">伴随方法 Adjoint Method</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9C%A8%E7%8A%B6%E6%80%81%E4%BC%B0%E8%AE%A1%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8-Application-in-State-Estimation"><span class="nav-number">5.</span> <span class="nav-text">在状态估计中的应用 Application in State Estimation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">6.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mengyin Liu</span>

  

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>








<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共99.0k字</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="总访客量">
      <i class="fa fa-user"></i>
      本站访客数<span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="总访问量">
      <i class="fa fa-eye"></i>
      总访问量<span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  















  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.0.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.0.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.0.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.0.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.4"></script>



  



	





  





  










  





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }},
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

    
  


  
  

  

  

  
  <script type="text/javascript" src="/js/src/exturl.js?v=6.0.4"></script>


  

</body>
</html>
