<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">











<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.0.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.4">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.4" color="#222">







<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Gemini',
    version: '6.0.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  
  <meta name="keywords" content="Hexo, NexT" />


<meta name="description" content="Mengyin Liu | 刘孟寅  PhD Candidate on Computer Vision PRIR Lab, University of Science and Technology Beijing  Research Interests: Vision and Language, Object Detection, AI4Science  News 2023&#x2F;03&#x2F;13: Our">
<meta property="og:type" content="website">
<meta property="og:title" content="Academic Resume">
<meta property="og:url" content="http://lmy98129.github.io/academic/index.html">
<meta property="og:site_name" content="NeXT">
<meta property="og:description" content="Mengyin Liu | 刘孟寅  PhD Candidate on Computer Vision PRIR Lab, University of Science and Technology Beijing  Research Interests: Vision and Language, Object Detection, AI4Science  News 2023&#x2F;03&#x2F;13: Our">
<meta property="og:locale">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/photo.jpg">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/TDSRFCC-figure1.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/TDSRFCC-figure2.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/VLPD-figure1.jpg">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/VLPD-figure2.jpg">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/CAliC-figure1.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/CAliC-figure2.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/AP2M-figure1.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/AP2M-figure2.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/USTB-logo.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/Tencent-logo.png">
<meta property="article:published_time" content="2020-12-09T10:43:47.000Z">
<meta property="article:modified_time" content="2023-04-01T07:41:23.777Z">
<meta property="article:author" content="Mengyin Liu">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://lmy98129.github.io/academic/src/photo.jpg">


  


  <link rel="alternate" href="/atom.xml" title="NeXT" type="application/atom+xml" />




  <link rel="canonical" href="http://lmy98129.github.io/academic/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>
  <title>Academic Resume | NeXT</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

<meta name="generator" content="Hexo 5.4.2"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">NeXT</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
        </li>
      
        
        <li class="menu-item menu-item-academic">
          <a href="/academic/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-flask"></i> <br />学术</a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />标签</a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>
        </li>
      

      
    </ul>
  

  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    
    
    <div class="post-block page">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">Academic Resume</h1>

<div class="post-meta">
  
  



</div>

</header>

      
      
      
      <div class="post-body">
        
        
          <p><img src="/academic/src/photo.jpg" alt="My Photo"></p>
<center>Mengyin Liu | 刘孟寅</center>

<center>PhD Candidate on Computer Vision <br>PRIR Lab, University of Science and Technology Beijing</center>

<center>Research Interests: Vision and Language, <br>Object Detection, AI4Science</center>

<h2 id="News"><a href="#News" class="headerlink" title="News"></a>News</h2><ul>
<li>2023/03/13: Our paper &quot;Towards Discriminative Semantic Relationship for Fine-grained Crowd Counting&quot; has been accepted by ICME&apos;23!</li>
<li>2023/02/28: Our paper &quot;VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision&quot; has been accepted by CVPR&apos;23!</li>
<li>2022/06/30: Our paper &quot;CAliC: Accurate and Efficient Image-Text Retrieval via Contrastive Alignment and Visual Contexts Modeling&quot; has been accepted by MM&apos;22!</li>
<li>2020/12/02: Our paper &quot;Adaptive Pattern-Parameter Matching for Robust Pedestrian Detection&quot; has been accepted by AAAI&apos;21!</li>
</ul>
<h2 id="Publications"><a href="#Publications" class="headerlink" title="Publications"></a>Publications</h2><h3 id="Towards-Discriminative-Semantic-Relationship-for-Fine-grained-Crowd-Counting-ICME-apos-23"><a href="#Towards-Discriminative-Semantic-Relationship-for-Fine-grained-Crowd-Counting-ICME-apos-23" class="headerlink" title="Towards Discriminative Semantic Relationship for Fine-grained Crowd Counting (ICME&apos;23)"></a>Towards Discriminative Semantic Relationship for Fine-grained Crowd Counting (ICME&apos;23)</h3><p>Shiqi Ren, Chao Zhu<sup>*</sup>, <strong>Mengyin Liu</strong>, Xu-Cheng Yin. University of Science and Technology Beijing.</p>
<p><img src="/academic/src/TDSRFCC-figure1.png" alt="TDSRFCC Fig1" class="full-img"></p>
<p><img src="/academic/src/TDSRFCC-figure2.png" alt="TDSRFCC Fig2" class="head-img"></p>
<p style="line-height: 1.5"><br><strong>Abstract</strong> As an extended task of crowd counting, fine-grained crowd counting aims to estimate the number of people in each semantic category instead of the whole in an image, and faces challenges including 1) inter-category crowd appearance similarity, 2) intra-category crowd appearance variations, and 3) frequent scene changes. In this paper, we propose a new fine-grained crowd counting approach named DSR to tackle these challenges by modeling Discriminative Semantic Relationship, which consists of two key components: Word Vector Module (WVM) and Adaptive Kernel Module (AKM). The WVM introduces more explicit semantic relationship information to better distinguish people of different semantic groups with similar appearance. The AKM dynamically adjusts kernel weights according to the features from different crowd appearance and scenes. The proposed DSR achieves superior results over state-of-the-art on the standard dataset. Our approach can serve as a new solid baseline and facilitate future research for the task of fine-grained crowd counting.<br></p>

<p>Resources: [Paper (Coming Soon)], [Supplementary Materials (Coming Soon)]</p>
<h3 id="VLPD-Context-Aware-Pedestrian-Detection-via-Vision-Language-Semantic-Self-Supervision-CVPR-apos-23"><a href="#VLPD-Context-Aware-Pedestrian-Detection-via-Vision-Language-Semantic-Self-Supervision-CVPR-apos-23" class="headerlink" title="VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision (CVPR&apos;23)"></a>VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision (CVPR&apos;23)</h3><p><strong>Mengyin Liu</strong><sup>1#</sup>, Jie Jiang<sup>2#</sup>, Chao Zhu<sup>1*</sup>, Xu-Cheng Yin<sup>1</sup>.<br><sup>1</sup>University of Science and Technology Beijing. <sup>2</sup>Data Platform Department, Tencent. (# Equal contribution)</p>
<p><img src="/academic/src/VLPD-figure1.jpg" alt="VLPD Fig1" class="full-img"></p>
<p><img src="/academic/src/VLPD-figure2.jpg" alt="VLPD Fig2" class="head-img"></p>
<p style="line-height: 1.5"><br><strong>Abstract</strong> Detecting pedestrians accurately in urban scenes is significant for realistic applications like autonomous driving or video surveillance. However, confusing human-like objects often lead to wrong detections, and small scale or heavily occluded pedestrians are easily missed due to their unusual appearances. To address these challenges, only object regions are inadequate, thus how to fully utilize more explicit and semantic contexts becomes a key problem. Meanwhile, previous context-aware pedestrian detectors either only learn latent contexts with visual clues, or need laborious annotations to obtain explicit and semantic contexts. Therefore, we propose in this paper a novel approach via Vision-Language semantic self-supervision for context-aware Pedestrian Detection (VLPD) to model explicitly semantic contexts without any extra annotations. Firstly, we propose a self-supervised Vision-Language Semantic (VLS) segmentation method, which learns both fully-supervised pedestrian detection and contextual segmentation via self-generated explicit labels of semantic classes by vision-language models. Furthermore, a self-supervised Prototypical Semantic Contrastive (PSC) learning method is proposed to better discriminate pedestrians and other classes, based on more explicit and semantic contexts obtained from VLS. Extensive experiments on popular benchmarks show that our proposed VLPD achieves superior performances over the previous state-of-the-arts, particularly under challenging circumstances like small scale and heavy occlusion.<br></p>

<p>Resources: [<a href="/academic/src/VLPD-Paper.pdf">Paper</a>], [<a href="/academic/src/VLPD-Supp.pdf">Supplementary Materials</a>], [<a target="_blank" rel="noopener" href="https://github.com/lmy98129/VLPD">Code</a>]</p>
<h3 id="CAliC-Accurate-and-Efficient-Image-Text-Retrieval-via-Contrastive-Alignment-and-Visual-Contexts-Modeling-MM-apos-22"><a href="#CAliC-Accurate-and-Efficient-Image-Text-Retrieval-via-Contrastive-Alignment-and-Visual-Contexts-Modeling-MM-apos-22" class="headerlink" title="CAliC: Accurate and Efficient Image-Text Retrieval via Contrastive Alignment and Visual Contexts Modeling (MM&apos;22)"></a>CAliC: Accurate and Efficient Image-Text Retrieval via Contrastive Alignment and Visual Contexts Modeling (MM&apos;22)</h3><p>Hongyu Gao<sup>1</sup>, Chao Zhu<sup>1*</sup>, <strong>Mengyin Liu</strong><sup>1</sup>, Weibo Gu<sup>2</sup>, Hongfa Wang<sup>2</sup>, Wei Liu<sup>2</sup>, Xu-Cheng Yin<sup>1</sup>.<br><sup>1</sup>University of Science and Technology Beijing. <sup>2</sup>Data Platform Department, Tencent. </p>
<p><img src="/academic/src/CAliC-figure1.png" alt="CAliC Fig2" class="full-img"></p>
<p><img src="/academic/src/CAliC-figure2.png" alt="CAliC Fig2" class="head-img"></p>
<p style="line-height: 1.5"><br><strong>Abstract</strong> Image-text retrieval is an essential task of information retrieval, in which the models with the Vision-and-Language Pretraining (VLP) are able to achieve ideal accuracy compared with the ones without VLP. Among different VLP approaches, the single-stream models achieve the overall best retrieval accuracy, but slower inference speed. Recently, researchers have introduced the two-stage retrieval setting commonly used in the information retrieval field to the single-stream VLP model for a better accuracy/efficiency trade-off. However, the retrieval accuracy and efficiency are still unsatisfactory mainly due to the limitations of the patch-based visual unimodal encoder in these VLP models. The unimodal encoders are trained on pure visual data, so the visual features extracted by them are difficult to align with the textual features and it is also difficult for the multi-modal encoder to understand visual information. Under these circumstances, we propose an accurate and efficient two-stage image-text retrieval model via Contrastive Alignment and visual Contexts modeling (CAliC). In the first stage of the proposed model, the visual unimodal encoder is pretrained with cross-modal contrastive learning to extract easily aligned visual features, which improves the retrieval accuracy and the inference speed. In the second stage of the proposed model, we introduce a new visual contexts modeling task during pretraining to help the multi-modal encoder better understand the visual information and get more accurate predictions. Extensive experimental evaluation validates the effectiveness of our proposed approach, which achieves a higher retrieval accuracy while keeping a faster inference speed, and outperforms existing state-of-the-art retrieval methods on image-text retrieval tasks over Flickr30K and COCO benchmarks.<br></p>

<p>Resources: [<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3503161.3548320">Paper</a>]</p>
<h3 id="Adaptive-Pattern-Parameter-Matching-for-Robust-Pedestrian-Detection-AAAI-apos-21"><a href="#Adaptive-Pattern-Parameter-Matching-for-Robust-Pedestrian-Detection-AAAI-apos-21" class="headerlink" title="Adaptive Pattern-Parameter Matching for Robust Pedestrian Detection (AAAI&apos;21)"></a>Adaptive Pattern-Parameter Matching for Robust Pedestrian Detection (AAAI&apos;21)</h3><p><strong>Mengyin Liu</strong>, Chao Zhu<sup>*</sup>, Jun Wang, Xu-Cheng Yin<sup>*</sup>. University of Science and Technology Beijing.</p>
<p><img src="/academic/src/AP2M-figure1.png" alt="AP2M Fig1" class="full-img"></p>
<p><img src="/academic/src/AP2M-figure2.png" alt="AP2M Fig2" class="head-img"></p>
<p style="line-height: 1.5"><br><strong>Abstract</strong>  Pedestrians with challenging patterns, e.g. small scale or heavy occlusion, appear frequently in practical applications like autonomous driving, which remains tremendous obstacle to higher robustness of detectors. Although plenty of previous works have been dedicated to these problems, properly matching patterns of pedestrian and parameters of detector, i.e., constructing a detector with proper parameter sizes for certain pedestrian patterns of different complexity, has been seldom investigated intensively. Pedestrian instances are usually handled equally with the same amount of parameters, which in our opinion is inadequate for those with more difficult patterns and leads to unsatisfactory performance. Thus, we propose in this paper a novel detection approach via adaptive pattern-parameter matching. The input pedestrian patterns, especially the complex ones, are first disentangled to simpler patterns by parallel branches in Pattern Disentangling Module (PDM) with various receptive fields. Then, Gating Feature Filtering Module (GFFM) dynamically decides the spatial positions where the patterns are still not simple enough and need further disentanglement by the next-level PDM. Cooperating with these two key components, our approach can adaptively select the best matched parameter size for the input patterns according to their complexity. Moreover, to further explore the relationship between parameter sizes and their performance on the corresponding patterns, two parameter selection policies are designed: 1) extending parameter size to maximum, aiming at more difficult patterns for different occlusion types; 2) specializing parameter size by group division, aiming at complex patterns for scale variations. Extensive experiments on two popular benchmarks, Caltech and CityPersons, show that our proposed method achieves superior performance compared with other state-of-the-art methods on subsets of different scales and occlusion types.<br></p>

<p>Resources: [<a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/16313">Paper</a>], [<a href="/academic/src/AP2M-Appendix.pdf">Appendix</a>], [<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1_egvWg-VDo7ouJneR-baPg">Codes&amp;Data</a> (Password: z6gg)]</p>
<h2 id="Education"><a href="#Education" class="headerlink" title="Education"></a>Education</h2><p><img src="/academic/src/USTB-logo.png" alt="USTB Logo" class="logo-img"></p>
<ul>
<li>2020/09~NOW: Master-Doctorate Degree Program on Computer Vision. Pattern Recognition and Information Retrieval Lab (PRIR Lab), University of Science and Technology Beijing</li>
<li>2016/09~2020/06: Bachelor Degree on Computer Science. School of Computer and Communication Engineering, University of Science and Technology Beijing</li>
</ul>
<h2 id="Employment"><a href="#Employment" class="headerlink" title="Employment"></a>Employment</h2><p><img src="/academic/src/Tencent-logo.png" alt="Tencent Logo" class="logo-img logo-long"></p>
<ul>
<li>2021/06~2022/01: Internship on Data Algorithm Engineering at Data Platform Department, Tencent. </li>
</ul>

        
      </div>
      
      
      
    </div>
    



    
    
    
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/%7C%7C%20archive">
                
                    <span class="site-state-item-count">7</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/lmy98129" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:blean@live.cn" target="_blank" title="Mail"><i class="fa fa-fw fa-envelope"></i>Mail</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="http://blog.csdn.net/lmy98129" target="_blank" title="CSDN"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/liu-meng-yin-68/" target="_blank" title="知乎"><i class="fa fa-fw fa-comment"></i>知乎</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#News"><span class="nav-number">1.</span> <span class="nav-text">News</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Publications"><span class="nav-number">2.</span> <span class="nav-text">Publications</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Towards-Discriminative-Semantic-Relationship-for-Fine-grained-Crowd-Counting-ICME-apos-23"><span class="nav-number">2.1.</span> <span class="nav-text">Towards Discriminative Semantic Relationship for Fine-grained Crowd Counting (ICME&#39;23)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VLPD-Context-Aware-Pedestrian-Detection-via-Vision-Language-Semantic-Self-Supervision-CVPR-apos-23"><span class="nav-number">2.2.</span> <span class="nav-text">VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision (CVPR&#39;23)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CAliC-Accurate-and-Efficient-Image-Text-Retrieval-via-Contrastive-Alignment-and-Visual-Contexts-Modeling-MM-apos-22"><span class="nav-number">2.3.</span> <span class="nav-text">CAliC: Accurate and Efficient Image-Text Retrieval via Contrastive Alignment and Visual Contexts Modeling (MM&#39;22)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adaptive-Pattern-Parameter-Matching-for-Robust-Pedestrian-Detection-AAAI-apos-21"><span class="nav-number">2.4.</span> <span class="nav-text">Adaptive Pattern-Parameter Matching for Robust Pedestrian Detection (AAAI&#39;21)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Education"><span class="nav-number">3.</span> <span class="nav-text">Education</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Employment"><span class="nav-number">4.</span> <span class="nav-text">Employment</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mengyin Liu</span>

  

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>








<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共54.2k字</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="总访客量">
      <i class="fa fa-user"></i>
      本站访客数<span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="总访问量">
      <i class="fa fa-eye"></i>
      总访问量<span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  















  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.0.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.0.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.0.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.0.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.4"></script>



  



	





  





  










  





  

  

  

  
  

  
  

  


  
  

  

  

  
  <script type="text/javascript" src="/js/src/exturl.js?v=6.0.4"></script>


  

</body>
</html>
