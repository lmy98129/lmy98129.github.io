<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">











<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.0.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.4">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.4" color="#222">







<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Gemini',
    version: '6.0.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  
  <meta name="keywords" content="Hexo, NexT" />


<meta name="description" content="Mengyin Liu | 刘孟寅  PhD Candidate on Computer Vision PRIR Lab, University of Science and Technology Beijing  Research Interests: Vision and Language, Object Detection, AI4Science  News 2025&#x2F;02&#x2F;15: Our">
<meta property="og:type" content="website">
<meta property="og:title" content="Academic Resume">
<meta property="og:url" content="http://lmy98129.github.io/academic/index.html">
<meta property="og:site_name" content="NeXT">
<meta property="og:description" content="Mengyin Liu | 刘孟寅  PhD Candidate on Computer Vision PRIR Lab, University of Science and Technology Beijing  Research Interests: Vision and Language, Object Detection, AI4Science  News 2025&#x2F;02&#x2F;15: Our">
<meta property="og:locale">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/photo.jpg">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/HA-FGOVD-figure1.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/HA-FGOVD-figure2.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/OSS-OCL-figure1.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/OSS-OCL-figure2.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/UMPD-figure1.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/UMPD-figure2.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/FIESR-figure1.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/FIESR-figure2.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/TDSRFCC-figure1.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/TDSRFCC-figure2.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/VLPD-figure1.jpg">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/VLPD-figure2.jpg">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/CAliC-figure1.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/CAliC-figure2.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/AP2M-figure1.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/AP2M-figure2.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/USTB-logo.png">
<meta property="og:image" content="http://lmy98129.github.io/academic/src/Tencent-logo.png">
<meta property="article:published_time" content="2020-12-09T10:43:47.000Z">
<meta property="article:modified_time" content="2025-04-05T10:48:09.300Z">
<meta property="article:author" content="Mengyin Liu">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://lmy98129.github.io/academic/src/photo.jpg">






  <link rel="canonical" href="http://lmy98129.github.io/academic/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>
  <title>Academic Resume | NeXT</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

<meta name="generator" content="Hexo 5.4.2"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">NeXT</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
        </li>
      
        
        <li class="menu-item menu-item-academic">
          <a href="/academic/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-flask"></i> <br />学术</a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />标签</a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />搜索</a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    
    
    <div class="post-block page">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">Academic Resume</h1>

<div class="post-meta">
  
  



</div>

</header>

      
      
      
      <div class="post-body">
        
        
          <p><img src="/academic/src/photo.jpg" alt="My Photo"></p>
<center>Mengyin Liu | 刘孟寅</center>

<center>PhD Candidate on Computer Vision <br>PRIR Lab, University of Science and Technology Beijing</center>

<center>Research Interests: Vision and Language, <br>Object Detection, AI4Science</center>

<h2 id="News"><a href="#News" class="headerlink" title="News"></a>News</h2><ul>
<li>2025/02/15: Our paper &quot;HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear Composition for Open-Vocabulary Object Detection&quot; is accepted by TMM&apos;25! </li>
<li>2025/01/23: Our paper &quot;OSS-OCL: Occlusion Scenario Simulation and Occluded-edge Concentrated Learning for Pedestrian Detection&quot; is accepted by PR Letters&apos;25!</li>
<li>2024/07/16: Our paper &quot;Unsupervised Multi-view Pedestrian Detection&quot; is accepted by ACM MM&apos;24!</li>
<li>2023/08/21: Our paper &quot;Feature Implicit Enhancement via Super-Resolution for Small Object Detection&quot; is accepted by PRCV&apos;23!</li>
<li>2023/06/08: Our paper &quot;Towards Discriminative Semantic Relationship for Fine-grained Crowd Counting&quot; is scheduled in ICME&apos;23 oral session! (Please refer to &quot;Tuesday 11th July&quot;→&quot;10:30 - 12:00&quot;→&quot;O2 - Semantic Processing I&quot; on the <a target="_blank" rel="noopener" href="https://www.2023.ieeeicme.org/program.php">official website</a>.)</li>
<li>2023/03/13: Our paper &quot;Towards Discriminative Semantic Relationship for Fine-grained Crowd Counting&quot; is accepted by ICME&apos;23!</li>
<li>2023/02/28: Our paper &quot;VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision&quot; is accepted by CVPR&apos;23!</li>
<li>2022/06/30: Our paper &quot;CAliC: Accurate and Efficient Image-Text Retrieval via Contrastive Alignment and Visual Contexts Modeling&quot; is accepted by ACM MM&apos;22!</li>
<li>2020/12/02: Our paper &quot;Adaptive Pattern-Parameter Matching for Robust Pedestrian Detection&quot; is accepted by AAAI&apos;21!</li>
</ul>
<h2 id="Publications"><a href="#Publications" class="headerlink" title="Publications"></a>Publications</h2><h3 id="HA-FGOVD-Highlighting-Fine-grained-Attributes-via-Explicit-Linear-Composition-for-Open-Vocabulary-Object-Detection-TMM-apos-25"><a href="#HA-FGOVD-Highlighting-Fine-grained-Attributes-via-Explicit-Linear-Composition-for-Open-Vocabulary-Object-Detection-TMM-apos-25" class="headerlink" title="HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear Composition for Open-Vocabulary Object Detection (TMM&apos;25)"></a>HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear Composition for Open-Vocabulary Object Detection (TMM&apos;25)</h3><p>Yuqi Ma<sup>#</sup>, <strong>Mengyin Liu</strong><sup>#</sup>, Chao Zhu<sup>*</sup>, Xu-Cheng Yin. University of Science and Technology Beijing. (# Equal contribution)</p>
<p><img src="/academic/src/HA-FGOVD-figure1.png" alt="HA-FGOVD Fig1" class="full-img"></p>
<p><img src="/academic/src/HA-FGOVD-figure2.png" alt="HA-FGOVD Fig2" class="head-img head-img-larger3"></p>
<p style="line-height: 1.5"><br>Open-vocabulary object detection (OVD) models are considered to be Large Multi-modal Models (LMM), due to their extensive training data and a large number of parameters. Main- stream OVD models prioritize object coarse-grained category rather than focus on their fine-grained attributes, e.g., colors or materials, thus failed to identify objects specified with certain attributes. Despite being pretrained on large-scale image-text pairs with rich attribute information, their latent feature space does not highlight these fine-grained attributes. In this paper, we introduce HA-FGOVD, a universal and explicit method that enhances the attribute-level detection capabilities of frozen OVD models by highlighting fine-grained attributes in explicit linear space. Our approach uses a LLM to extract attribute words in input text as a zero-shot task. Then, token attention masks are adjusted to guide text encoders in extracting both global and attribute-specific features, which are explicitly composited as two vectors in linear space to form a new attribute-highlighted feature for detection tasks. The composition weight scalars can be learned or transferred across different OVD models, showcasing the universality of our method. Experimental results show that HA-FGOVD achieves state-of-the-art performance on the FG- OVD benchmark and demonstrates promising generalization on the OVDEval benchmark, suggesting that our method addresses significant limitations in fine-grained attribute detection and has potential for broader fine-grained detection applications.<br></p>

<p>Resources: [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10948367">Paper</a>]</p>
<h3 id="OSS-OCL-Occlusion-Scenario-Simulation-and-Occluded-edge-Concentrated-Learning-for-Pedestrian-Detection-PR-Letters-apos-25"><a href="#OSS-OCL-Occlusion-Scenario-Simulation-and-Occluded-edge-Concentrated-Learning-for-Pedestrian-Detection-PR-Letters-apos-25" class="headerlink" title="OSS-OCL: Occlusion Scenario Simulation and Occluded-edge Concentrated Learning for Pedestrian Detection (PR Letters&apos;25)"></a>OSS-OCL: Occlusion Scenario Simulation and Occluded-edge Concentrated Learning for Pedestrian Detection (PR Letters&apos;25)</h3><p>Keqi Lu<sup>1,2</sup>, Chao Zhu<sup>2*</sup>, <strong>Mengyin Liu</strong><sup>2</sup>, Xu-Cheng Yin<sup>2</sup>.<br><sup>1</sup>Children&apos;s Hospital, Zhejiang University School of Medicine. <sup>2</sup>University of Science and Technology Beijing. </p>
<p><img src="/academic/src/OSS-OCL-figure1.png" alt="OSS-OCL Fig1" class="full-img"></p>
<p><img src="/academic/src/OSS-OCL-figure2.png" alt="OSS-OCL Fig2" class="head-img head-img-larger2"></p>
<p style="line-height: 1.5"><br><strong>Abstract</strong> Pedestrian detection plays an important role in realistic applications. However, heavily occluded pedestrians, with their incomplete and unusual appearances, are easily missed during detection. To address this issue, previous works use copy-paste method or generate dummies to assist the detectors in learning better detection capability. Nevertheless, these works focus on less frequent occlusion in natural scenes and lead to less performance gain. Therefore, we firstly propose a novel method named Occlusion Scenario Simulation (OSS), which simulates the most classical occlusion scenario by inserting objects adjacent to the non- or partial-occluded pedestrians. Secondly, in order to supervise the detector to better learn the occlusion information, we also propose a new method namely Occluded-edge Concentrated Learning (OCL) to predict the offset of occluded-edge between pedestrians and occlusions. Extensive experiments on popular pedestrian datasets demonstrate that our proposed OSS-OCL outperforms some state-of-the-art methods, particularly in the cases of heavy occlusion.<br></p>

<p>Resources: [<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0167865525000261">Paper</a>]</p>
<h3 id="Unsupervised-Multi-view-Pedestrian-Detection-ACM-MM-apos-24"><a href="#Unsupervised-Multi-view-Pedestrian-Detection-ACM-MM-apos-24" class="headerlink" title="Unsupervised Multi-view Pedestrian Detection (ACM MM&apos;24)"></a>Unsupervised Multi-view Pedestrian Detection (ACM MM&apos;24)</h3><p><strong>Mengyin Liu</strong>, Chao Zhu<sup>*</sup>, Shiqi Ren, Xu-Cheng Yin. University of Science and Technology Beijing.</p>
<p><img src="/academic/src/UMPD-figure1.png" alt="UMPD Fig1" class="full-img"></p>
<p><img src="/academic/src/UMPD-figure2.png" alt="UMPD Fig2" class="head-img head-img-larger"></p>
<p style="line-height: 1.5"><br><strong>Abstract</strong> With the prosperity of the intelligent surveillance, multiple cameras have been applied to localize pedestrians more accurately. However, previous methods rely on laborious annotations of pedestrians in every frame and camera view. Therefore, we propose in this paper an Unsupervised Multi-view Pedestrian Detection approach (UMPD) to learn an annotation-free detector via vision-language models and 2D-3D cross-modal mapping: 1) Firstly, Semantic-aware Iterative Segmentation (SIS) is proposed to extract unsupervised representations of multi-view images, which are converted into 2D masks as pseudo labels, via our proposed iterative PCA and zero-shot semantic classes from vision-language models; 2) Secondly, we propose Geometry-aware Volume-based Detector (GVD) to end-to-end encode multi-view 2D images into a 3D volume to predict voxel-wise density and color via 2D-to-3D geometric projection, trained by 3D-to-2D rendering losses with SIS pseudo labels; 3) Thirdly, for better detection results, i.e., the 3D density projected on Birds-Eye-View, we propose Vertical-aware BEV Regularization (VBR) to constrain pedestrians to be vertical like the natural poses.  Extensive experiments on popular multi-view pedestrian detection benchmarks Wildtrack, Terrace, and MultiviewX, show that our proposed UMPD, as the first fully-unsupervised method to our best knowledge, performs competitively to the previous state-of-the-art supervised methods.<br></p>

<p>Resources: [<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3664647.3681560">Paper</a>], [<a href="/academic/src/UMPD-Appendix.pdf">Supplementary Materials</a>], [<a target="_blank" rel="noopener" href="https://github.com/lmy98129/UMPD">Code</a>]</p>
<h3 id="Feature-Implicit-Enhancement-via-Super-Resolution-for-Small-Object-Detection-PRCV-apos-23"><a href="#Feature-Implicit-Enhancement-via-Super-Resolution-for-Small-Object-Detection-PRCV-apos-23" class="headerlink" title="Feature Implicit Enhancement via Super-Resolution for Small Object Detection (PRCV&apos;23)"></a>Feature Implicit Enhancement via Super-Resolution for Small Object Detection (PRCV&apos;23)</h3><p>Zhehao Xu, <strong>Mengyin Liu</strong>, Chao Zhu<sup>*</sup>, Fang Zhou, Xu-Cheng Yin. University of Science and Technology Beijing.</p>
<p><img src="/academic/src/FIESR-figure1.png" alt="FIESR Fig1" class="full-img"></p>
<p><img src="/academic/src/FIESR-figure2.png" alt="FIESR Fig2" class="head-img"></p>
<p style="line-height: 1.5"><br><strong>Abstract</strong> In recent years, object detection has made significant strides due to advancements in deep convolutional neural networks. However, the detection performance for small objects remains challenging. The visual information of small objects is easily confused with the background and even gets lost in a series of downsampling operations due to the limited number of pixels, resulting in poor representations. In this paper, we propose a novel approach namely Feature Implicit Enhancement via Super-Resolution (FIESR) to learn more robust feature representations for small object detection. Our FIESR consists of two detection branches and requires two steps of training. Firstly, the detector learns the relationship between low-resolution and corresponding original high-resolution images to enhance the representations of small objects by minimizing a super-resolution loss between the two branches. Secondly, the detector is fine-tuned on original resolution images to fit extremely large objects. Additionally, our FIESR could be applied to various popular detectors such as Faster-RCNN, RetinaNet, FCOS, and DyHead. Our FIESR achieves competitive results on COCO dataset and is proved effective and flexible by extensive experiments.<br></p>

<p>Resources: [<a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-981-99-8555-5_11">Paper</a>]</p>
<h3 id="Towards-Discriminative-Semantic-Relationship-for-Fine-grained-Crowd-Counting-ICME-apos-23-oral"><a href="#Towards-Discriminative-Semantic-Relationship-for-Fine-grained-Crowd-Counting-ICME-apos-23-oral" class="headerlink" title="Towards Discriminative Semantic Relationship for Fine-grained Crowd Counting (ICME&apos;23 oral)"></a>Towards Discriminative Semantic Relationship for Fine-grained Crowd Counting (ICME&apos;23 oral)</h3><p>Shiqi Ren, Chao Zhu<sup>*</sup>, <strong>Mengyin Liu</strong>, Xu-Cheng Yin. University of Science and Technology Beijing.</p>
<p><img src="/academic/src/TDSRFCC-figure1.png" alt="TDSRFCC Fig1" class="full-img"></p>
<p><img src="/academic/src/TDSRFCC-figure2.png" alt="TDSRFCC Fig2" class="head-img"></p>
<p style="line-height: 1.5"><br><strong>Abstract</strong> As an extended task of crowd counting, fine-grained crowd counting aims to estimate the number of people in each semantic category instead of the whole in an image, and faces challenges including 1) inter-category crowd appearance similarity, 2) intra-category crowd appearance variations, and 3) frequent scene changes. In this paper, we propose a new fine-grained crowd counting approach named DSR to tackle these challenges by modeling Discriminative Semantic Relationship, which consists of two key components: Word Vector Module (WVM) and Adaptive Kernel Module (AKM). The WVM introduces more explicit semantic relationship information to better distinguish people of different semantic groups with similar appearance. The AKM dynamically adjusts kernel weights according to the features from different crowd appearance and scenes. The proposed DSR achieves superior results over state-of-the-art on the standard dataset. Our approach can serve as a new solid baseline and facilitate future research for the task of fine-grained crowd counting.<br></p>

<p>Resources: [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10219788">Paper</a>], [<a href="/academic/src/TDSRFCC-Supp.pdf">Supplementary Materials</a>]</p>
<h3 id="VLPD-Context-Aware-Pedestrian-Detection-via-Vision-Language-Semantic-Self-Supervision-CVPR-apos-23"><a href="#VLPD-Context-Aware-Pedestrian-Detection-via-Vision-Language-Semantic-Self-Supervision-CVPR-apos-23" class="headerlink" title="VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision (CVPR&apos;23)"></a>VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision (CVPR&apos;23)</h3><p><strong>Mengyin Liu</strong><sup>1#</sup>, Jie Jiang<sup>2#</sup>, Chao Zhu<sup>1*</sup>, Xu-Cheng Yin<sup>1</sup>.<br><sup>1</sup>University of Science and Technology Beijing. <sup>2</sup>Data Platform Department, Tencent. (# Equal contribution)</p>
<p><img src="/academic/src/VLPD-figure1.jpg" alt="VLPD Fig1" class="full-img"></p>
<p><img src="/academic/src/VLPD-figure2.jpg" alt="VLPD Fig2" class="head-img"></p>
<p style="line-height: 1.5"><br><strong>Abstract</strong> Detecting pedestrians accurately in urban scenes is significant for realistic applications like autonomous driving or video surveillance. However, confusing human-like objects often lead to wrong detections, and small scale or heavily occluded pedestrians are easily missed due to their unusual appearances. To address these challenges, only object regions are inadequate, thus how to fully utilize more explicit and semantic contexts becomes a key problem. Meanwhile, previous context-aware pedestrian detectors either only learn latent contexts with visual clues, or need laborious annotations to obtain explicit and semantic contexts. Therefore, we propose in this paper a novel approach via Vision-Language semantic self-supervision for context-aware Pedestrian Detection (VLPD) to model explicitly semantic contexts without any extra annotations. Firstly, we propose a self-supervised Vision-Language Semantic (VLS) segmentation method, which learns both fully-supervised pedestrian detection and contextual segmentation via self-generated explicit labels of semantic classes by vision-language models. Furthermore, a self-supervised Prototypical Semantic Contrastive (PSC) learning method is proposed to better discriminate pedestrians and other classes, based on more explicit and semantic contexts obtained from VLS. Extensive experiments on popular benchmarks show that our proposed VLPD achieves superior performances over the previous state-of-the-arts, particularly under challenging circumstances like small scale and heavy occlusion.<br></p>

<p>Resources: [<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_VLPD_Context-Aware_Pedestrian_Detection_via_Vision-Language_Semantic_Self-Supervision_CVPR_2023_paper.html">Paper</a>], [<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Liu_VLPD_Context-Aware_Pedestrian_CVPR_2023_supplemental.pdf">Supplementary Materials</a>], [<a target="_blank" rel="noopener" href="https://github.com/lmy98129/VLPD">Code</a>]</p>
<h3 id="CAliC-Accurate-and-Efficient-Image-Text-Retrieval-via-Contrastive-Alignment-and-Visual-Contexts-Modeling-ACM-MM-apos-22"><a href="#CAliC-Accurate-and-Efficient-Image-Text-Retrieval-via-Contrastive-Alignment-and-Visual-Contexts-Modeling-ACM-MM-apos-22" class="headerlink" title="CAliC: Accurate and Efficient Image-Text Retrieval via Contrastive Alignment and Visual Contexts Modeling (ACM MM&apos;22)"></a>CAliC: Accurate and Efficient Image-Text Retrieval via Contrastive Alignment and Visual Contexts Modeling (ACM MM&apos;22)</h3><p>Hongyu Gao<sup>1</sup>, Chao Zhu<sup>1*</sup>, <strong>Mengyin Liu</strong><sup>1</sup>, Weibo Gu<sup>2</sup>, Hongfa Wang<sup>2</sup>, Wei Liu<sup>2</sup>, Xu-Cheng Yin<sup>1</sup>.<br><sup>1</sup>University of Science and Technology Beijing. <sup>2</sup>Data Platform Department, Tencent. </p>
<p><img src="/academic/src/CAliC-figure1.png" alt="CAliC Fig2" class="full-img"></p>
<p><img src="/academic/src/CAliC-figure2.png" alt="CAliC Fig2" class="head-img"></p>
<p style="line-height: 1.5"><br><strong>Abstract</strong> Image-text retrieval is an essential task of information retrieval, in which the models with the Vision-and-Language Pretraining (VLP) are able to achieve ideal accuracy compared with the ones without VLP. Among different VLP approaches, the single-stream models achieve the overall best retrieval accuracy, but slower inference speed. Recently, researchers have introduced the two-stage retrieval setting commonly used in the information retrieval field to the single-stream VLP model for a better accuracy/efficiency trade-off. However, the retrieval accuracy and efficiency are still unsatisfactory mainly due to the limitations of the patch-based visual unimodal encoder in these VLP models. The unimodal encoders are trained on pure visual data, so the visual features extracted by them are difficult to align with the textual features and it is also difficult for the multi-modal encoder to understand visual information. Under these circumstances, we propose an accurate and efficient two-stage image-text retrieval model via Contrastive Alignment and visual Contexts modeling (CAliC). In the first stage of the proposed model, the visual unimodal encoder is pretrained with cross-modal contrastive learning to extract easily aligned visual features, which improves the retrieval accuracy and the inference speed. In the second stage of the proposed model, we introduce a new visual contexts modeling task during pretraining to help the multi-modal encoder better understand the visual information and get more accurate predictions. Extensive experimental evaluation validates the effectiveness of our proposed approach, which achieves a higher retrieval accuracy while keeping a faster inference speed, and outperforms existing state-of-the-art retrieval methods on image-text retrieval tasks over Flickr30K and COCO benchmarks.<br></p>

<p>Resources: [<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3503161.3548320">Paper</a>], [<a href="/academic/src/CAliC-Supp.pdf">Supplementary Materials</a>]</p>
<h3 id="Adaptive-Pattern-Parameter-Matching-for-Robust-Pedestrian-Detection-AAAI-apos-21"><a href="#Adaptive-Pattern-Parameter-Matching-for-Robust-Pedestrian-Detection-AAAI-apos-21" class="headerlink" title="Adaptive Pattern-Parameter Matching for Robust Pedestrian Detection (AAAI&apos;21)"></a>Adaptive Pattern-Parameter Matching for Robust Pedestrian Detection (AAAI&apos;21)</h3><p><strong>Mengyin Liu</strong>, Chao Zhu<sup>*</sup>, Jun Wang, Xu-Cheng Yin<sup>*</sup>. University of Science and Technology Beijing.</p>
<p><img src="/academic/src/AP2M-figure1.png" alt="AP2M Fig1" class="full-img"></p>
<p><img src="/academic/src/AP2M-figure2.png" alt="AP2M Fig2" class="head-img"></p>
<p style="line-height: 1.5"><br><strong>Abstract</strong>  Pedestrians with challenging patterns, e.g. small scale or heavy occlusion, appear frequently in practical applications like autonomous driving, which remains tremendous obstacle to higher robustness of detectors. Although plenty of previous works have been dedicated to these problems, properly matching patterns of pedestrian and parameters of detector, i.e., constructing a detector with proper parameter sizes for certain pedestrian patterns of different complexity, has been seldom investigated intensively. Pedestrian instances are usually handled equally with the same amount of parameters, which in our opinion is inadequate for those with more difficult patterns and leads to unsatisfactory performance. Thus, we propose in this paper a novel detection approach via adaptive pattern-parameter matching. The input pedestrian patterns, especially the complex ones, are first disentangled to simpler patterns by parallel branches in Pattern Disentangling Module (PDM) with various receptive fields. Then, Gating Feature Filtering Module (GFFM) dynamically decides the spatial positions where the patterns are still not simple enough and need further disentanglement by the next-level PDM. Cooperating with these two key components, our approach can adaptively select the best matched parameter size for the input patterns according to their complexity. Moreover, to further explore the relationship between parameter sizes and their performance on the corresponding patterns, two parameter selection policies are designed: 1) extending parameter size to maximum, aiming at more difficult patterns for different occlusion types; 2) specializing parameter size by group division, aiming at complex patterns for scale variations. Extensive experiments on two popular benchmarks, Caltech and CityPersons, show that our proposed method achieves superior performance compared with other state-of-the-art methods on subsets of different scales and occlusion types.<br></p>

<p>Resources: [<a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/16313">Paper</a>], [<a href="/academic/src/AP2M-Appendix.pdf">Appendix</a>], [<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1_egvWg-VDo7ouJneR-baPg">Code&amp;Data</a> (Password: z6gg)]</p>
<h2 id="Education"><a href="#Education" class="headerlink" title="Education"></a>Education</h2><p><img src="/academic/src/USTB-logo.png" alt="USTB Logo" class="logo-img"></p>
<ul>
<li>2020/09~NOW: Master-Doctorate Degree Program on Computer Vision. Pattern Recognition and Information Retrieval Lab (PRIR Lab), University of Science and Technology Beijing.</li>
<li>2016/09~2020/06: Bachelor Degree on Computer Science and Technology. School of Computer and Communication Engineering, University of Science and Technology Beijing.</li>
</ul>
<h2 id="Work-Experiences"><a href="#Work-Experiences" class="headerlink" title="Work Experiences"></a>Work Experiences</h2><p><img src="/academic/src/Tencent-logo.png" alt="Tencent Logo" class="logo-img logo-long"></p>
<ul>
<li>2021/06~2022/01: Internship on Data Algorithm Engineering at Data Platform Department, Tencent. <!-- - Conference/Journal Paper Reviewer: IROS&apos;23, ACM MM&apos;23, CVPR&apos;24; CVIU, TMM. --></li>
</ul>

        
      </div>
      
      
      
    </div>
    



    
    
    
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Mengyin Liu" />
            
              <p class="site-author-name" itemprop="name">Mengyin Liu</p>
              <p class="site-description motion-element" itemprop="description">Stay hungry, stay foolish</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">15</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/lmy98129" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:blean@live.cn" target="_blank" title="Mail"><i class="fa fa-fw fa-envelope"></i>Mail</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://scholar.google.com/citations?user=hN7koAYAAAAJ&hl=zh-CN" target="_blank" title="Scholar"><i class="fa fa-fw fa-google"></i>Scholar</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/liu-meng-yin-68/" target="_blank" title="知乎"><i class="fa fa-fw fa-comment"></i>知乎</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#News"><span class="nav-number">1.</span> <span class="nav-text">News</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Publications"><span class="nav-number">2.</span> <span class="nav-text">Publications</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#HA-FGOVD-Highlighting-Fine-grained-Attributes-via-Explicit-Linear-Composition-for-Open-Vocabulary-Object-Detection-TMM-apos-25"><span class="nav-number">2.1.</span> <span class="nav-text">HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear Composition for Open-Vocabulary Object Detection (TMM&#39;25)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#OSS-OCL-Occlusion-Scenario-Simulation-and-Occluded-edge-Concentrated-Learning-for-Pedestrian-Detection-PR-Letters-apos-25"><span class="nav-number">2.2.</span> <span class="nav-text">OSS-OCL: Occlusion Scenario Simulation and Occluded-edge Concentrated Learning for Pedestrian Detection (PR Letters&#39;25)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Unsupervised-Multi-view-Pedestrian-Detection-ACM-MM-apos-24"><span class="nav-number">2.3.</span> <span class="nav-text">Unsupervised Multi-view Pedestrian Detection (ACM MM&#39;24)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Feature-Implicit-Enhancement-via-Super-Resolution-for-Small-Object-Detection-PRCV-apos-23"><span class="nav-number">2.4.</span> <span class="nav-text">Feature Implicit Enhancement via Super-Resolution for Small Object Detection (PRCV&#39;23)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Towards-Discriminative-Semantic-Relationship-for-Fine-grained-Crowd-Counting-ICME-apos-23-oral"><span class="nav-number">2.5.</span> <span class="nav-text">Towards Discriminative Semantic Relationship for Fine-grained Crowd Counting (ICME&#39;23 oral)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VLPD-Context-Aware-Pedestrian-Detection-via-Vision-Language-Semantic-Self-Supervision-CVPR-apos-23"><span class="nav-number">2.6.</span> <span class="nav-text">VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision (CVPR&#39;23)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CAliC-Accurate-and-Efficient-Image-Text-Retrieval-via-Contrastive-Alignment-and-Visual-Contexts-Modeling-ACM-MM-apos-22"><span class="nav-number">2.7.</span> <span class="nav-text">CAliC: Accurate and Efficient Image-Text Retrieval via Contrastive Alignment and Visual Contexts Modeling (ACM MM&#39;22)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adaptive-Pattern-Parameter-Matching-for-Robust-Pedestrian-Detection-AAAI-apos-21"><span class="nav-number">2.8.</span> <span class="nav-text">Adaptive Pattern-Parameter Matching for Robust Pedestrian Detection (AAAI&#39;21)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Education"><span class="nav-number">3.</span> <span class="nav-text">Education</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Work-Experiences"><span class="nav-number">4.</span> <span class="nav-text">Work Experiences</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mengyin Liu</span>

  

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>








<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共99.0k字</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="总访客量">
      <i class="fa fa-user"></i>
      本站访客数<span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="总访问量">
      <i class="fa fa-eye"></i>
      总访问量<span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  















  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.0.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.0.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.0.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.0.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.4"></script>



  



	





  





  










  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  

  


  
  

  

  

  
  <script type="text/javascript" src="/js/src/exturl.js?v=6.0.4"></script>


  

</body>
</html>
